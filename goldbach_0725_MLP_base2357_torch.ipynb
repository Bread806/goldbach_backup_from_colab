{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE/UiV6lHNFm2n2ZzC8GW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bread806/goldbach_backup_from_colab/blob/main/goldbach_0725_MLP_base2357_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9qnR_PN5EFK",
        "outputId": "8c98e0ad-21ce-4326-e039-4efa0a121592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def is_prime(num):\n",
        "    \"\"\"檢查一個數字是否為質數\"\"\"\n",
        "    if num <= 1:\n",
        "        return False\n",
        "    for i in range(2, int(num ** 0.5) + 1):\n",
        "        if num % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def prime_table(x):\n",
        "    \"\"\"建立小於x的質數表\"\"\"\n",
        "    primes = [num for num in range(2, x) if is_prime(num)]\n",
        "    return primes\n",
        "\n",
        "\n",
        "def convert_base_into_list(number, base, width=10):\n",
        "    result = []  # init list\n",
        "\n",
        "    for i in range(width):\n",
        "        result.append([number % base])\n",
        "        number = number // base\n",
        "    result = result[::-1]\n",
        "    return result\n",
        "\n",
        "\n",
        "def prime_to_index(primeSize, primes, number):\n",
        "    for index in range(primeSize):\n",
        "        if number == primes[index]:\n",
        "            return index\n",
        "    return -1"
      ],
      "metadata": {
        "id": "DxVObRdp_GSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaEH2nRv1EY2",
        "outputId": "4006d36e-1f3f-43ae-afba-39e3c60a8dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---loading data---\n",
            "---loading data done.---\n",
            "--- Using device: cpu ---\n",
            "Input size: 40\n",
            "---starting training---\n",
            "Epoch [1/1000], Train Loss: 167613974.8571, Val Loss: 188751944.0000\n",
            "Epoch [2/1000], Train Loss: 161889922.2857, Val Loss: 188155576.0000\n",
            "Epoch [3/1000], Train Loss: 159069301.7143, Val Loss: 185008976.0000\n",
            "Epoch [4/1000], Train Loss: 160607721.1429, Val Loss: 172922424.0000\n",
            "Epoch [5/1000], Train Loss: 135719798.8571, Val Loss: 137804416.0000\n",
            "Epoch [6/1000], Train Loss: 93300369.1429, Val Loss: 73394532.0000\n",
            "Epoch [7/1000], Train Loss: 51032174.2857, Val Loss: 67498308.0000\n",
            "Epoch [8/1000], Train Loss: 50581839.1429, Val Loss: 55794224.0000\n",
            "Epoch [9/1000], Train Loss: 44515348.0000, Val Loss: 60065304.0000\n",
            "Epoch [10/1000], Train Loss: 43720569.7143, Val Loss: 54496184.0000\n",
            "Epoch [11/1000], Train Loss: 42660554.2857, Val Loss: 52551620.0000\n",
            "Epoch [12/1000], Train Loss: 40398246.0000, Val Loss: 51309208.0000\n",
            "Epoch [13/1000], Train Loss: 40666639.1429, Val Loss: 51183832.0000\n",
            "Epoch [14/1000], Train Loss: 37308629.1429, Val Loss: 49081136.0000\n",
            "Epoch [15/1000], Train Loss: 37885082.8571, Val Loss: 47599854.0000\n",
            "Epoch [16/1000], Train Loss: 38143295.1429, Val Loss: 46506976.0000\n",
            "Epoch [17/1000], Train Loss: 34864102.2857, Val Loss: 44776713.0000\n",
            "Epoch [18/1000], Train Loss: 33620188.5714, Val Loss: 43560176.0000\n",
            "Epoch [19/1000], Train Loss: 31552153.4286, Val Loss: 41866048.0000\n",
            "Epoch [20/1000], Train Loss: 30650028.0000, Val Loss: 40230646.0000\n",
            "Epoch [21/1000], Train Loss: 30827278.2857, Val Loss: 38039415.0000\n",
            "Epoch [22/1000], Train Loss: 27149779.4286, Val Loss: 35895819.0000\n",
            "Epoch [23/1000], Train Loss: 27288655.7143, Val Loss: 33768932.0000\n",
            "Epoch [24/1000], Train Loss: 25325324.5714, Val Loss: 31366004.0000\n",
            "Epoch [25/1000], Train Loss: 21665033.1429, Val Loss: 29084692.0000\n",
            "Epoch [26/1000], Train Loss: 20085211.0000, Val Loss: 27068774.0000\n",
            "Epoch [27/1000], Train Loss: 19079762.2857, Val Loss: 24533575.0000\n",
            "Epoch [28/1000], Train Loss: 16652489.2857, Val Loss: 22735963.0000\n",
            "Epoch [29/1000], Train Loss: 15203707.7143, Val Loss: 20839089.0000\n",
            "Epoch [30/1000], Train Loss: 13791020.8571, Val Loss: 19282925.5000\n",
            "Epoch [31/1000], Train Loss: 13432470.2857, Val Loss: 17791897.5000\n",
            "Epoch [32/1000], Train Loss: 12343852.4286, Val Loss: 16645326.5000\n",
            "Epoch [33/1000], Train Loss: 11838530.4286, Val Loss: 15400541.5000\n",
            "Epoch [34/1000], Train Loss: 11307685.2143, Val Loss: 14771575.5000\n",
            "Epoch [35/1000], Train Loss: 10053451.7857, Val Loss: 14042158.5000\n",
            "Epoch [36/1000], Train Loss: 10034034.2857, Val Loss: 13536948.5000\n",
            "Epoch [37/1000], Train Loss: 9409458.0714, Val Loss: 13061394.5000\n",
            "Epoch [38/1000], Train Loss: 9106322.6429, Val Loss: 12635799.0000\n",
            "Epoch [39/1000], Train Loss: 9485722.0000, Val Loss: 12355522.0000\n",
            "Epoch [40/1000], Train Loss: 8281739.6429, Val Loss: 12042007.5000\n",
            "Epoch [41/1000], Train Loss: 8114562.3571, Val Loss: 11618779.0000\n",
            "Epoch [42/1000], Train Loss: 8028467.0714, Val Loss: 11418752.0000\n",
            "Epoch [43/1000], Train Loss: 8074181.7143, Val Loss: 11116112.0000\n",
            "Epoch [44/1000], Train Loss: 7870083.5000, Val Loss: 10957839.0000\n",
            "Epoch [45/1000], Train Loss: 8194591.0714, Val Loss: 10899625.0000\n",
            "Epoch [46/1000], Train Loss: 7793135.7857, Val Loss: 10502595.0000\n",
            "Epoch [47/1000], Train Loss: 7069306.7143, Val Loss: 10644314.0000\n",
            "Epoch [48/1000], Train Loss: 6786262.2143, Val Loss: 10146160.0000\n",
            "Epoch [49/1000], Train Loss: 6948938.2857, Val Loss: 9970554.0000\n",
            "Epoch [50/1000], Train Loss: 7014227.1429, Val Loss: 9801860.5000\n",
            "Epoch [51/1000], Train Loss: 6860325.2857, Val Loss: 9607186.0000\n",
            "Epoch [52/1000], Train Loss: 6640997.7143, Val Loss: 9468442.0000\n",
            "Epoch [53/1000], Train Loss: 6812397.5714, Val Loss: 9352272.5000\n",
            "Epoch [54/1000], Train Loss: 6451608.2143, Val Loss: 9348378.5000\n",
            "Epoch [55/1000], Train Loss: 6244695.5000, Val Loss: 9641598.5000\n",
            "Epoch [56/1000], Train Loss: 6661933.4286, Val Loss: 9119953.0000\n",
            "Epoch [57/1000], Train Loss: 6367246.1429, Val Loss: 9009662.5000\n",
            "Epoch [58/1000], Train Loss: 6293412.0714, Val Loss: 8796761.0000\n",
            "Epoch [59/1000], Train Loss: 5933195.0000, Val Loss: 8662759.5000\n",
            "Epoch [60/1000], Train Loss: 5812141.2857, Val Loss: 8663518.0000\n",
            "Epoch [61/1000], Train Loss: 6411438.1429, Val Loss: 8593316.5000\n",
            "Epoch [62/1000], Train Loss: 5676378.9286, Val Loss: 8669487.0000\n",
            "Epoch [63/1000], Train Loss: 5653964.7143, Val Loss: 8423184.2500\n",
            "Epoch [64/1000], Train Loss: 5749431.5714, Val Loss: 8345238.2500\n",
            "Epoch [65/1000], Train Loss: 5580716.1786, Val Loss: 8415709.2500\n",
            "Epoch [66/1000], Train Loss: 5424068.4286, Val Loss: 8233013.5000\n",
            "Epoch [67/1000], Train Loss: 5396961.8214, Val Loss: 8149203.0000\n",
            "Epoch [68/1000], Train Loss: 5108095.1071, Val Loss: 8227672.7500\n",
            "Epoch [69/1000], Train Loss: 5361536.6429, Val Loss: 8306809.7500\n",
            "Epoch [70/1000], Train Loss: 5284519.3214, Val Loss: 8378384.5000\n",
            "Epoch [71/1000], Train Loss: 5217585.6429, Val Loss: 7990562.0000\n",
            "Epoch [72/1000], Train Loss: 5055837.8214, Val Loss: 7916413.7500\n",
            "Epoch [73/1000], Train Loss: 5327106.1429, Val Loss: 7773427.2500\n",
            "Epoch [74/1000], Train Loss: 5245754.6786, Val Loss: 7725918.7500\n",
            "Epoch [75/1000], Train Loss: 4945475.6429, Val Loss: 7618900.2500\n",
            "Epoch [76/1000], Train Loss: 4930921.6786, Val Loss: 7530979.2500\n",
            "Epoch [77/1000], Train Loss: 4684851.7500, Val Loss: 7492023.7500\n",
            "Epoch [78/1000], Train Loss: 4609021.1071, Val Loss: 7449934.0000\n",
            "Epoch [79/1000], Train Loss: 4499777.7143, Val Loss: 7405023.5000\n",
            "Epoch [80/1000], Train Loss: 4456528.5000, Val Loss: 7327279.0000\n",
            "Epoch [81/1000], Train Loss: 4272924.0714, Val Loss: 7281218.5000\n",
            "Epoch [82/1000], Train Loss: 4487564.8571, Val Loss: 7430859.5000\n",
            "Epoch [83/1000], Train Loss: 4085975.0714, Val Loss: 7206478.5000\n",
            "Epoch [84/1000], Train Loss: 4287949.7143, Val Loss: 7053704.7500\n",
            "Epoch [85/1000], Train Loss: 4166433.3214, Val Loss: 6932174.2500\n",
            "Epoch [86/1000], Train Loss: 3984286.5357, Val Loss: 7015473.7500\n",
            "Epoch [87/1000], Train Loss: 4207148.3929, Val Loss: 6744726.7500\n",
            "Epoch [88/1000], Train Loss: 4294378.3214, Val Loss: 6661754.0000\n",
            "Epoch [89/1000], Train Loss: 3939667.4286, Val Loss: 6640436.2500\n",
            "Epoch [90/1000], Train Loss: 3781872.1786, Val Loss: 6581062.7500\n",
            "Epoch [91/1000], Train Loss: 3718633.6429, Val Loss: 6535413.2500\n",
            "Epoch [92/1000], Train Loss: 3459994.8214, Val Loss: 6477762.2500\n",
            "Epoch [93/1000], Train Loss: 3527822.8571, Val Loss: 6571808.2500\n",
            "Epoch [94/1000], Train Loss: 3356200.6429, Val Loss: 6452472.5000\n",
            "Epoch [95/1000], Train Loss: 3372062.7857, Val Loss: 6314085.0000\n",
            "Epoch [96/1000], Train Loss: 3126415.9821, Val Loss: 6344995.7500\n",
            "Epoch [97/1000], Train Loss: 3342348.7143, Val Loss: 6193731.2500\n",
            "Epoch [98/1000], Train Loss: 3079074.6071, Val Loss: 6114740.2500\n",
            "Epoch [99/1000], Train Loss: 2926199.8929, Val Loss: 6049294.2500\n",
            "Epoch [100/1000], Train Loss: 2966889.4286, Val Loss: 5952174.5000\n",
            "Epoch [101/1000], Train Loss: 2965068.9643, Val Loss: 5901459.0000\n",
            "Epoch [102/1000], Train Loss: 2830322.8571, Val Loss: 5823618.2500\n",
            "Epoch [103/1000], Train Loss: 2656270.1071, Val Loss: 5735113.0000\n",
            "Epoch [104/1000], Train Loss: 2502551.8750, Val Loss: 5919855.0000\n",
            "Epoch [105/1000], Train Loss: 2502737.0714, Val Loss: 5768504.2500\n",
            "Epoch [106/1000], Train Loss: 2481403.6964, Val Loss: 5720362.5000\n",
            "Epoch [107/1000], Train Loss: 2402571.9286, Val Loss: 5411904.0000\n",
            "Epoch [108/1000], Train Loss: 2539955.1250, Val Loss: 5405833.5000\n",
            "Epoch [109/1000], Train Loss: 2349838.1607, Val Loss: 5477042.5000\n",
            "Epoch [110/1000], Train Loss: 2407087.4107, Val Loss: 5209866.5000\n",
            "Epoch [111/1000], Train Loss: 2301227.4821, Val Loss: 5184865.2500\n",
            "Epoch [112/1000], Train Loss: 2133220.7321, Val Loss: 5077099.5000\n",
            "Epoch [113/1000], Train Loss: 1972943.8393, Val Loss: 5042716.5000\n",
            "Epoch [114/1000], Train Loss: 1961826.3036, Val Loss: 5057809.5000\n",
            "Epoch [115/1000], Train Loss: 1872342.8393, Val Loss: 5045672.0000\n",
            "Epoch [116/1000], Train Loss: 1816373.6161, Val Loss: 4987305.2500\n",
            "Epoch [117/1000], Train Loss: 1875202.1786, Val Loss: 4824259.5000\n",
            "Epoch [118/1000], Train Loss: 1737764.0893, Val Loss: 4786622.2500\n",
            "Epoch [119/1000], Train Loss: 1699197.4464, Val Loss: 4717328.7500\n",
            "Epoch [120/1000], Train Loss: 1666118.1250, Val Loss: 5027748.5000\n",
            "Epoch [121/1000], Train Loss: 1705334.8929, Val Loss: 4651359.7500\n",
            "Epoch [122/1000], Train Loss: 1681384.9821, Val Loss: 4668037.1250\n",
            "Epoch [123/1000], Train Loss: 1562865.4286, Val Loss: 4572858.2500\n",
            "Epoch [124/1000], Train Loss: 1548982.9107, Val Loss: 4566215.6250\n",
            "Epoch [125/1000], Train Loss: 1488727.8125, Val Loss: 4463114.0000\n",
            "Epoch [126/1000], Train Loss: 1493248.3393, Val Loss: 4389980.6250\n",
            "Epoch [127/1000], Train Loss: 1317042.0714, Val Loss: 4349997.6250\n",
            "Epoch [128/1000], Train Loss: 1313191.8036, Val Loss: 4303119.6250\n",
            "Epoch [129/1000], Train Loss: 1283570.7411, Val Loss: 4219848.1250\n",
            "Epoch [130/1000], Train Loss: 1370869.3750, Val Loss: 4217849.8750\n",
            "Epoch [131/1000], Train Loss: 1262017.4554, Val Loss: 4276463.0000\n",
            "Epoch [132/1000], Train Loss: 1180548.5268, Val Loss: 4090629.2500\n",
            "Epoch [133/1000], Train Loss: 1183621.4107, Val Loss: 4124217.3750\n",
            "Epoch [134/1000], Train Loss: 1236914.6607, Val Loss: 4078947.0000\n",
            "Epoch [135/1000], Train Loss: 1170208.2321, Val Loss: 4071459.6250\n",
            "Epoch [136/1000], Train Loss: 1218790.5179, Val Loss: 4206493.5000\n",
            "Epoch [137/1000], Train Loss: 1100044.5536, Val Loss: 4045622.5000\n",
            "Epoch [138/1000], Train Loss: 1105919.5893, Val Loss: 3967241.3750\n",
            "Epoch [139/1000], Train Loss: 985773.8750, Val Loss: 3970203.3750\n",
            "Epoch [140/1000], Train Loss: 1028385.7054, Val Loss: 4002365.2500\n",
            "Epoch [141/1000], Train Loss: 1066171.8482, Val Loss: 3962987.8750\n",
            "Epoch [142/1000], Train Loss: 953880.9554, Val Loss: 3890433.7500\n",
            "Epoch [143/1000], Train Loss: 931681.1339, Val Loss: 3878603.3750\n",
            "Epoch [144/1000], Train Loss: 927682.1964, Val Loss: 3941279.2500\n",
            "Epoch [145/1000], Train Loss: 949348.7857, Val Loss: 3842388.5000\n",
            "Epoch [146/1000], Train Loss: 905215.5804, Val Loss: 3853286.0000\n",
            "Epoch [147/1000], Train Loss: 917638.8929, Val Loss: 3826015.5000\n",
            "Epoch [148/1000], Train Loss: 915211.0357, Val Loss: 3813788.7500\n",
            "Epoch [149/1000], Train Loss: 914357.1429, Val Loss: 3921937.2500\n",
            "Epoch [150/1000], Train Loss: 931324.5982, Val Loss: 4173341.6250\n",
            "Epoch [151/1000], Train Loss: 991493.3125, Val Loss: 3856024.0000\n",
            "Epoch [152/1000], Train Loss: 915348.7143, Val Loss: 3805114.1250\n",
            "Epoch [153/1000], Train Loss: 951007.8036, Val Loss: 3726138.5000\n",
            "Epoch [154/1000], Train Loss: 851643.1518, Val Loss: 3749058.2500\n",
            "Epoch [155/1000], Train Loss: 829230.8036, Val Loss: 3857809.7500\n",
            "Epoch [156/1000], Train Loss: 850408.8214, Val Loss: 3991208.6250\n",
            "Epoch [157/1000], Train Loss: 926079.4286, Val Loss: 3839053.2500\n",
            "Epoch [158/1000], Train Loss: 825959.9643, Val Loss: 3764463.5000\n",
            "Epoch [159/1000], Train Loss: 894822.4196, Val Loss: 3745542.6250\n",
            "Epoch [160/1000], Train Loss: 820804.0268, Val Loss: 3710379.6250\n",
            "Epoch [161/1000], Train Loss: 738879.2679, Val Loss: 3659328.6250\n",
            "Epoch [162/1000], Train Loss: 757348.2679, Val Loss: 3707117.7500\n",
            "Epoch [163/1000], Train Loss: 749195.3393, Val Loss: 3621994.6250\n",
            "Epoch [164/1000], Train Loss: 720689.9062, Val Loss: 3742506.0000\n",
            "Epoch [165/1000], Train Loss: 707379.4821, Val Loss: 3611529.0000\n",
            "Epoch [166/1000], Train Loss: 662044.0268, Val Loss: 3629350.3750\n",
            "Epoch [167/1000], Train Loss: 673850.9062, Val Loss: 3684190.3750\n",
            "Epoch [168/1000], Train Loss: 663947.9821, Val Loss: 3634421.0000\n",
            "Epoch [169/1000], Train Loss: 629785.7098, Val Loss: 3603343.3750\n",
            "Epoch [170/1000], Train Loss: 642262.2723, Val Loss: 3574833.0000\n",
            "Epoch [171/1000], Train Loss: 622754.7634, Val Loss: 3579680.5000\n",
            "Epoch [172/1000], Train Loss: 625007.8036, Val Loss: 3620352.6250\n",
            "Epoch [173/1000], Train Loss: 640409.6696, Val Loss: 3573285.3750\n",
            "Epoch [174/1000], Train Loss: 579064.6071, Val Loss: 3568574.7500\n",
            "Epoch [175/1000], Train Loss: 599776.9018, Val Loss: 3568775.0000\n",
            "Epoch [176/1000], Train Loss: 587309.7321, Val Loss: 3558803.6250\n",
            "Epoch [177/1000], Train Loss: 559400.8125, Val Loss: 3568827.5000\n",
            "Epoch [178/1000], Train Loss: 588047.9955, Val Loss: 3572318.8750\n",
            "Epoch [179/1000], Train Loss: 566180.2946, Val Loss: 3606488.1250\n",
            "Epoch [180/1000], Train Loss: 560109.7009, Val Loss: 3537423.7500\n",
            "Epoch [181/1000], Train Loss: 527197.6027, Val Loss: 3450116.8750\n",
            "Epoch [182/1000], Train Loss: 519838.5938, Val Loss: 3448045.3750\n",
            "Epoch [183/1000], Train Loss: 496043.4018, Val Loss: 3447482.1250\n",
            "Epoch [184/1000], Train Loss: 512650.5089, Val Loss: 3496888.7500\n",
            "Epoch [185/1000], Train Loss: 508427.0938, Val Loss: 3428632.8750\n",
            "Epoch [186/1000], Train Loss: 514326.7545, Val Loss: 3447026.7500\n",
            "Epoch [187/1000], Train Loss: 514258.1071, Val Loss: 3500661.1250\n",
            "Epoch [188/1000], Train Loss: 474954.5580, Val Loss: 3643659.6250\n",
            "Epoch [189/1000], Train Loss: 504588.1830, Val Loss: 3679158.6250\n",
            "Epoch [190/1000], Train Loss: 557403.0402, Val Loss: 3571820.2500\n",
            "Epoch [191/1000], Train Loss: 530776.1295, Val Loss: 3473118.5000\n",
            "Epoch [192/1000], Train Loss: 453546.6786, Val Loss: 3454143.3750\n",
            "Epoch [193/1000], Train Loss: 467278.5179, Val Loss: 3477580.0000\n",
            "Epoch [194/1000], Train Loss: 455643.1295, Val Loss: 3482480.5000\n",
            "Epoch [195/1000], Train Loss: 444366.9777, Val Loss: 3483568.5000\n",
            "Epoch [196/1000], Train Loss: 474831.5446, Val Loss: 3445493.6250\n",
            "Epoch [197/1000], Train Loss: 454024.3393, Val Loss: 3453943.5000\n",
            "Epoch [198/1000], Train Loss: 426450.3080, Val Loss: 3469336.7500\n",
            "Epoch [199/1000], Train Loss: 419657.7768, Val Loss: 3436340.6250\n",
            "Epoch [200/1000], Train Loss: 418866.0179, Val Loss: 3448854.6250\n",
            "Epoch [201/1000], Train Loss: 396867.7946, Val Loss: 3424585.3750\n",
            "Epoch [202/1000], Train Loss: 407269.6295, Val Loss: 3590513.5000\n",
            "Epoch [203/1000], Train Loss: 442151.1607, Val Loss: 3638288.5000\n",
            "Epoch [204/1000], Train Loss: 443067.4732, Val Loss: 3484969.5000\n",
            "Epoch [205/1000], Train Loss: 372874.7031, Val Loss: 3417137.1250\n",
            "Epoch [206/1000], Train Loss: 380710.2143, Val Loss: 3462588.1250\n",
            "Epoch [207/1000], Train Loss: 387448.7054, Val Loss: 3435248.8750\n",
            "Epoch [208/1000], Train Loss: 358070.5580, Val Loss: 3392442.3750\n",
            "Epoch [209/1000], Train Loss: 360434.2165, Val Loss: 3388855.1250\n",
            "Epoch [210/1000], Train Loss: 339108.3616, Val Loss: 3382331.2500\n",
            "Epoch [211/1000], Train Loss: 377786.3705, Val Loss: 3391572.6250\n",
            "Epoch [212/1000], Train Loss: 349532.4509, Val Loss: 3379622.8750\n",
            "Epoch [213/1000], Train Loss: 383302.5714, Val Loss: 3490071.0000\n",
            "Epoch [214/1000], Train Loss: 462969.3839, Val Loss: 3506505.5000\n",
            "Epoch [215/1000], Train Loss: 429531.9286, Val Loss: 3405632.3750\n",
            "Epoch [216/1000], Train Loss: 377544.7634, Val Loss: 3413255.0000\n",
            "Epoch [217/1000], Train Loss: 346988.4420, Val Loss: 3374231.8750\n",
            "Epoch [218/1000], Train Loss: 318344.2991, Val Loss: 3342098.1250\n",
            "Epoch [219/1000], Train Loss: 298882.5491, Val Loss: 3326001.7500\n",
            "Epoch [220/1000], Train Loss: 308633.1183, Val Loss: 3317372.1250\n",
            "Epoch [221/1000], Train Loss: 282545.9308, Val Loss: 3376839.7500\n",
            "Epoch [222/1000], Train Loss: 308706.7411, Val Loss: 3388144.6250\n",
            "Epoch [223/1000], Train Loss: 311612.4643, Val Loss: 3350467.0000\n",
            "Epoch [224/1000], Train Loss: 327681.1696, Val Loss: 3331173.0000\n",
            "Epoch [225/1000], Train Loss: 285744.4732, Val Loss: 3346251.6250\n",
            "Epoch [226/1000], Train Loss: 278038.2545, Val Loss: 3340147.2500\n",
            "Epoch [227/1000], Train Loss: 271761.5960, Val Loss: 3491626.1250\n",
            "Epoch [228/1000], Train Loss: 303414.0134, Val Loss: 3342543.6250\n",
            "Epoch [229/1000], Train Loss: 258551.2299, Val Loss: 3308470.8750\n",
            "Epoch [230/1000], Train Loss: 297147.5424, Val Loss: 3327049.3750\n",
            "Epoch [231/1000], Train Loss: 274678.8438, Val Loss: 3301989.1250\n",
            "Epoch [232/1000], Train Loss: 281171.5379, Val Loss: 3323342.0000\n",
            "Epoch [233/1000], Train Loss: 274296.2679, Val Loss: 3272889.1250\n",
            "Epoch [234/1000], Train Loss: 256394.9062, Val Loss: 3295451.7500\n",
            "Epoch [235/1000], Train Loss: 249298.1004, Val Loss: 3251063.6250\n",
            "Epoch [236/1000], Train Loss: 263572.2054, Val Loss: 3262870.1250\n",
            "Epoch [237/1000], Train Loss: 239661.0871, Val Loss: 3247912.1250\n",
            "Epoch [238/1000], Train Loss: 253442.6741, Val Loss: 3281005.5000\n",
            "Epoch [239/1000], Train Loss: 260384.3348, Val Loss: 3273039.5000\n",
            "Epoch [240/1000], Train Loss: 258526.8371, Val Loss: 3284585.3750\n",
            "Epoch [241/1000], Train Loss: 244032.5513, Val Loss: 3255518.1250\n",
            "Epoch [242/1000], Train Loss: 234311.6585, Val Loss: 3233673.5000\n",
            "Epoch [243/1000], Train Loss: 234892.0625, Val Loss: 3267439.1250\n",
            "Epoch [244/1000], Train Loss: 220642.7500, Val Loss: 3253079.0000\n",
            "Epoch [245/1000], Train Loss: 213296.0714, Val Loss: 3260841.0000\n",
            "Epoch [246/1000], Train Loss: 207354.9777, Val Loss: 3234375.6250\n",
            "Epoch [247/1000], Train Loss: 196984.4911, Val Loss: 3205884.7500\n",
            "Epoch [248/1000], Train Loss: 198235.1339, Val Loss: 3219722.0000\n",
            "Epoch [249/1000], Train Loss: 202708.9353, Val Loss: 3221772.5000\n",
            "Epoch [250/1000], Train Loss: 235434.8929, Val Loss: 3222514.5000\n",
            "Epoch [251/1000], Train Loss: 223659.5357, Val Loss: 3184937.5000\n",
            "Epoch [252/1000], Train Loss: 193409.1183, Val Loss: 3220373.0000\n",
            "Epoch [253/1000], Train Loss: 183114.2533, Val Loss: 3199562.6250\n",
            "Epoch [254/1000], Train Loss: 185061.1540, Val Loss: 3169756.6250\n",
            "Epoch [255/1000], Train Loss: 219562.1786, Val Loss: 3225973.1250\n",
            "Epoch [256/1000], Train Loss: 204066.2701, Val Loss: 3234953.0000\n",
            "Epoch [257/1000], Train Loss: 197255.8013, Val Loss: 3281403.8750\n",
            "Epoch [258/1000], Train Loss: 186372.7723, Val Loss: 3282101.2500\n",
            "Epoch [259/1000], Train Loss: 191882.8839, Val Loss: 3189681.7500\n",
            "Epoch [260/1000], Train Loss: 169203.1138, Val Loss: 3192908.5000\n",
            "Epoch [261/1000], Train Loss: 160233.4498, Val Loss: 3167723.5000\n",
            "Epoch [262/1000], Train Loss: 160849.4040, Val Loss: 3171548.1250\n",
            "Epoch [263/1000], Train Loss: 209570.0246, Val Loss: 3180250.3750\n",
            "Epoch [264/1000], Train Loss: 189663.4509, Val Loss: 3308131.8750\n",
            "Epoch [265/1000], Train Loss: 223567.3058, Val Loss: 3242262.3750\n",
            "Epoch [266/1000], Train Loss: 194179.8772, Val Loss: 3170020.6250\n",
            "Epoch [267/1000], Train Loss: 195136.2522, Val Loss: 3184272.1250\n",
            "Epoch [268/1000], Train Loss: 159497.2679, Val Loss: 3206274.8750\n",
            "Epoch [269/1000], Train Loss: 145833.2020, Val Loss: 3193099.6250\n",
            "Epoch [270/1000], Train Loss: 140561.2679, Val Loss: 3151756.5000\n",
            "Epoch [271/1000], Train Loss: 165897.7556, Val Loss: 3174770.6250\n",
            "Epoch [272/1000], Train Loss: 164247.6562, Val Loss: 3257404.0000\n",
            "Epoch [273/1000], Train Loss: 154025.3504, Val Loss: 3299820.7500\n",
            "Epoch [274/1000], Train Loss: 195131.5871, Val Loss: 3208621.3750\n",
            "Epoch [275/1000], Train Loss: 143811.4509, Val Loss: 3130657.0000\n",
            "Epoch [276/1000], Train Loss: 134944.8638, Val Loss: 3119199.0000\n",
            "Epoch [277/1000], Train Loss: 131793.5301, Val Loss: 3141191.3750\n",
            "Epoch [278/1000], Train Loss: 142148.4944, Val Loss: 3264592.2500\n",
            "Epoch [279/1000], Train Loss: 153840.0904, Val Loss: 3166198.2500\n",
            "Epoch [280/1000], Train Loss: 135819.5000, Val Loss: 3187433.6250\n",
            "Epoch [281/1000], Train Loss: 137234.5625, Val Loss: 3148693.8750\n",
            "Epoch [282/1000], Train Loss: 122848.2645, Val Loss: 3118615.7500\n",
            "Epoch [283/1000], Train Loss: 120875.6473, Val Loss: 3129755.0000\n",
            "Epoch [284/1000], Train Loss: 138530.6038, Val Loss: 3126985.5000\n",
            "Epoch [285/1000], Train Loss: 136161.8080, Val Loss: 3122062.5000\n",
            "Epoch [286/1000], Train Loss: 126648.3828, Val Loss: 3108513.1250\n",
            "Epoch [287/1000], Train Loss: 119915.2042, Val Loss: 3097138.7500\n",
            "Epoch [288/1000], Train Loss: 123097.4051, Val Loss: 3116484.2500\n",
            "Epoch [289/1000], Train Loss: 133591.3248, Val Loss: 3107641.0000\n",
            "Epoch [290/1000], Train Loss: 119385.9023, Val Loss: 3130224.6250\n",
            "Epoch [291/1000], Train Loss: 119132.1138, Val Loss: 3142628.6250\n",
            "Epoch [292/1000], Train Loss: 116585.2522, Val Loss: 3170790.0000\n",
            "Epoch [293/1000], Train Loss: 112177.3415, Val Loss: 3138988.0000\n",
            "Epoch [294/1000], Train Loss: 108027.3002, Val Loss: 3095210.2500\n",
            "Epoch [295/1000], Train Loss: 134605.4587, Val Loss: 3108976.7500\n",
            "Epoch [296/1000], Train Loss: 136924.4609, Val Loss: 3099689.7500\n",
            "Epoch [297/1000], Train Loss: 116021.4888, Val Loss: 3121549.3750\n",
            "Epoch [298/1000], Train Loss: 106646.6172, Val Loss: 3088861.7500\n",
            "Epoch [299/1000], Train Loss: 102148.0279, Val Loss: 3155244.2500\n",
            "Epoch [300/1000], Train Loss: 108382.7824, Val Loss: 3116222.2500\n",
            "Epoch [301/1000], Train Loss: 110875.2243, Val Loss: 3137075.0000\n",
            "Epoch [302/1000], Train Loss: 113939.8471, Val Loss: 3133206.8750\n",
            "Epoch [303/1000], Train Loss: 127815.5536, Val Loss: 3088287.8750\n",
            "Epoch [304/1000], Train Loss: 93118.4113, Val Loss: 3177818.6250\n",
            "Epoch [305/1000], Train Loss: 99594.9498, Val Loss: 3163776.7500\n",
            "Epoch [306/1000], Train Loss: 109893.8242, Val Loss: 3084621.7500\n",
            "Epoch [307/1000], Train Loss: 93002.4241, Val Loss: 3117672.2500\n",
            "Epoch [308/1000], Train Loss: 122398.1741, Val Loss: 3134929.5000\n",
            "Epoch [309/1000], Train Loss: 93424.3315, Val Loss: 3185356.6250\n",
            "Epoch [310/1000], Train Loss: 92078.3940, Val Loss: 3178944.3750\n",
            "Epoch [311/1000], Train Loss: 88918.0993, Val Loss: 3126815.8750\n",
            "Epoch [312/1000], Train Loss: 79774.5056, Val Loss: 3086381.8750\n",
            "Epoch [313/1000], Train Loss: 83175.2662, Val Loss: 3078020.3750\n",
            "Epoch [314/1000], Train Loss: 78889.1830, Val Loss: 3081221.8750\n",
            "Epoch [315/1000], Train Loss: 74622.8069, Val Loss: 3072654.7500\n",
            "Epoch [316/1000], Train Loss: 78188.1579, Val Loss: 3046953.2500\n",
            "Epoch [317/1000], Train Loss: 77061.2232, Val Loss: 3056021.0000\n",
            "Epoch [318/1000], Train Loss: 73600.1713, Val Loss: 3088215.6250\n",
            "Epoch [319/1000], Train Loss: 75829.5273, Val Loss: 3089600.7500\n",
            "Epoch [320/1000], Train Loss: 84320.4816, Val Loss: 3047124.8750\n",
            "Epoch [321/1000], Train Loss: 77461.4609, Val Loss: 3055972.1250\n",
            "Epoch [322/1000], Train Loss: 78319.7009, Val Loss: 3151142.0000\n",
            "Epoch [323/1000], Train Loss: 80290.8086, Val Loss: 3050355.7500\n",
            "Epoch [324/1000], Train Loss: 74934.2483, Val Loss: 3067623.6250\n",
            "Epoch [325/1000], Train Loss: 66502.6585, Val Loss: 3095098.3750\n",
            "Epoch [326/1000], Train Loss: 68986.8677, Val Loss: 3148208.1250\n",
            "Epoch [327/1000], Train Loss: 79970.1434, Val Loss: 3091291.0000\n",
            "Epoch [328/1000], Train Loss: 71074.5982, Val Loss: 3070852.2500\n",
            "Epoch [329/1000], Train Loss: 63273.1747, Val Loss: 3092481.5000\n",
            "Epoch [330/1000], Train Loss: 69174.9280, Val Loss: 3096945.3750\n",
            "Epoch [331/1000], Train Loss: 62708.5497, Val Loss: 3078778.8750\n",
            "Epoch [332/1000], Train Loss: 58216.9247, Val Loss: 3079755.0000\n",
            "Epoch [333/1000], Train Loss: 59739.7790, Val Loss: 3071552.2500\n",
            "Epoch [334/1000], Train Loss: 58773.9040, Val Loss: 3094844.3750\n",
            "Epoch [335/1000], Train Loss: 59093.6150, Val Loss: 3043962.5000\n",
            "Epoch [336/1000], Train Loss: 53162.4046, Val Loss: 3114523.8750\n",
            "Epoch [337/1000], Train Loss: 54368.3326, Val Loss: 3115222.2500\n",
            "Epoch [338/1000], Train Loss: 66101.5452, Val Loss: 3048881.1250\n",
            "Epoch [339/1000], Train Loss: 64059.2584, Val Loss: 3052928.7500\n",
            "Epoch [340/1000], Train Loss: 66056.2617, Val Loss: 3068663.5000\n",
            "Epoch [341/1000], Train Loss: 56875.5614, Val Loss: 3065448.8750\n",
            "Epoch [342/1000], Train Loss: 61524.7081, Val Loss: 3046170.0000\n",
            "Epoch [343/1000], Train Loss: 55546.8823, Val Loss: 3099418.2500\n",
            "Epoch [344/1000], Train Loss: 55679.7427, Val Loss: 3040982.8750\n",
            "Epoch [345/1000], Train Loss: 65455.1395, Val Loss: 3033406.8750\n",
            "Epoch [346/1000], Train Loss: 62695.2263, Val Loss: 3115287.0000\n",
            "Epoch [347/1000], Train Loss: 57695.3923, Val Loss: 3064545.7500\n",
            "Epoch [348/1000], Train Loss: 69781.1646, Val Loss: 3084712.3750\n",
            "Epoch [349/1000], Train Loss: 66856.1155, Val Loss: 3063511.3750\n",
            "Epoch [350/1000], Train Loss: 54279.9453, Val Loss: 3055952.3750\n",
            "Epoch [351/1000], Train Loss: 55899.3945, Val Loss: 3083413.5000\n",
            "Epoch [352/1000], Train Loss: 46558.0619, Val Loss: 3082837.5000\n",
            "Epoch [353/1000], Train Loss: 48329.2533, Val Loss: 3060304.6250\n",
            "Epoch [354/1000], Train Loss: 43361.2958, Val Loss: 3046526.5000\n",
            "Epoch [355/1000], Train Loss: 47405.5921, Val Loss: 3077024.5000\n",
            "Epoch [356/1000], Train Loss: 40051.2154, Val Loss: 3095100.7500\n",
            "Epoch [357/1000], Train Loss: 40402.0338, Val Loss: 3055761.5000\n",
            "Epoch [358/1000], Train Loss: 37145.4766, Val Loss: 3054936.8750\n",
            "Epoch [359/1000], Train Loss: 36937.2602, Val Loss: 3053744.0000\n",
            "Epoch [360/1000], Train Loss: 37151.7835, Val Loss: 3046862.8750\n",
            "Epoch [361/1000], Train Loss: 37038.9283, Val Loss: 3113956.8750\n",
            "Epoch [362/1000], Train Loss: 43348.5965, Val Loss: 3058596.3750\n",
            "Epoch [363/1000], Train Loss: 44928.3039, Val Loss: 3069749.0000\n",
            "Epoch [364/1000], Train Loss: 44181.0324, Val Loss: 3112111.2500\n",
            "Epoch [365/1000], Train Loss: 39046.4509, Val Loss: 3052349.7500\n",
            "Epoch [366/1000], Train Loss: 35626.5818, Val Loss: 3085227.5000\n",
            "Epoch [367/1000], Train Loss: 39896.0561, Val Loss: 3057988.6250\n",
            "Epoch [368/1000], Train Loss: 45020.0689, Val Loss: 3106092.5000\n",
            "Epoch [369/1000], Train Loss: 41351.3108, Val Loss: 3074245.3750\n",
            "Epoch [370/1000], Train Loss: 37469.0619, Val Loss: 3085411.5000\n",
            "Epoch [371/1000], Train Loss: 44445.0653, Val Loss: 3057172.0000\n",
            "Epoch [372/1000], Train Loss: 31423.3368, Val Loss: 3057318.6250\n",
            "Epoch [373/1000], Train Loss: 47528.3926, Val Loss: 3241662.5000\n",
            "Epoch [374/1000], Train Loss: 71530.2734, Val Loss: 3085035.5000\n",
            "Epoch [375/1000], Train Loss: 57553.0140, Val Loss: 3113098.6250\n",
            "Epoch [376/1000], Train Loss: 45353.4297, Val Loss: 3069499.0000\n",
            "Epoch [377/1000], Train Loss: 35786.1465, Val Loss: 3049846.1250\n",
            "Epoch [378/1000], Train Loss: 37748.0647, Val Loss: 3094554.0000\n",
            "Epoch [379/1000], Train Loss: 36768.5114, Val Loss: 3046018.5000\n",
            "Epoch [380/1000], Train Loss: 33848.4146, Val Loss: 3058033.5000\n",
            "Epoch [381/1000], Train Loss: 30500.5589, Val Loss: 3038966.5000\n",
            "Epoch [382/1000], Train Loss: 29980.8622, Val Loss: 3111951.0000\n",
            "Epoch [383/1000], Train Loss: 35931.8566, Val Loss: 3034527.2500\n",
            "Epoch [384/1000], Train Loss: 42477.7821, Val Loss: 3101020.1250\n",
            "Epoch [385/1000], Train Loss: 34428.0558, Val Loss: 3046744.6250\n",
            "Epoch [386/1000], Train Loss: 29127.3535, Val Loss: 3103326.7500\n",
            "Epoch [387/1000], Train Loss: 25675.1607, Val Loss: 3041219.6250\n",
            "Epoch [388/1000], Train Loss: 25327.7162, Val Loss: 3080978.5000\n",
            "Epoch [389/1000], Train Loss: 25813.3412, Val Loss: 3045535.6250\n",
            "Epoch [390/1000], Train Loss: 30294.6454, Val Loss: 3102521.6250\n",
            "Epoch [391/1000], Train Loss: 28780.4489, Val Loss: 3052231.0000\n",
            "Epoch [392/1000], Train Loss: 29528.9604, Val Loss: 3086499.5000\n",
            "Epoch [393/1000], Train Loss: 22090.0564, Val Loss: 3108459.2500\n",
            "Epoch [394/1000], Train Loss: 24049.4523, Val Loss: 3034777.0000\n",
            "Epoch [395/1000], Train Loss: 25205.4082, Val Loss: 3092007.2500\n",
            "Epoch [396/1000], Train Loss: 25050.8555, Val Loss: 3048486.2500\n",
            "Epoch [397/1000], Train Loss: 18825.1769, Val Loss: 3062362.8750\n",
            "Epoch [398/1000], Train Loss: 19779.6212, Val Loss: 3066783.3750\n",
            "Epoch [399/1000], Train Loss: 19590.5831, Val Loss: 3063141.6250\n",
            "Epoch [400/1000], Train Loss: 18302.5589, Val Loss: 3062742.7500\n",
            "Epoch [401/1000], Train Loss: 18779.5797, Val Loss: 3054456.6250\n",
            "Epoch [402/1000], Train Loss: 18298.2447, Val Loss: 3074617.8750\n",
            "Epoch [403/1000], Train Loss: 20430.6306, Val Loss: 3047280.8750\n",
            "Epoch [404/1000], Train Loss: 19625.8597, Val Loss: 3072172.0000\n",
            "Epoch [405/1000], Train Loss: 18776.7373, Val Loss: 3065693.6250\n",
            "Epoch [406/1000], Train Loss: 19823.5905, Val Loss: 3038267.7500\n",
            "Epoch [407/1000], Train Loss: 21545.6115, Val Loss: 3118628.5000\n",
            "Epoch [408/1000], Train Loss: 22005.0085, Val Loss: 3067107.8750\n",
            "Epoch [409/1000], Train Loss: 15809.8066, Val Loss: 3078549.3750\n",
            "Epoch [410/1000], Train Loss: 19572.0340, Val Loss: 3046696.1250\n",
            "Epoch [411/1000], Train Loss: 17197.1628, Val Loss: 3067036.3750\n",
            "Epoch [412/1000], Train Loss: 15794.4408, Val Loss: 3056990.1250\n",
            "Epoch [413/1000], Train Loss: 20946.4968, Val Loss: 3080505.7500\n",
            "Epoch [414/1000], Train Loss: 18780.7699, Val Loss: 3088594.6250\n",
            "Epoch [415/1000], Train Loss: 22640.9950, Val Loss: 3052927.2500\n",
            "Epoch [416/1000], Train Loss: 22119.9805, Val Loss: 3107314.5000\n",
            "Epoch [417/1000], Train Loss: 20316.1881, Val Loss: 3065525.8750\n",
            "Epoch [418/1000], Train Loss: 14043.1881, Val Loss: 3083854.1250\n",
            "Epoch [419/1000], Train Loss: 13967.3163, Val Loss: 3051467.1250\n",
            "Epoch [420/1000], Train Loss: 19821.0522, Val Loss: 3110556.6250\n",
            "Epoch [421/1000], Train Loss: 14876.9499, Val Loss: 3064524.2500\n",
            "Epoch [422/1000], Train Loss: 16164.5007, Val Loss: 3078148.5000\n",
            "Epoch [423/1000], Train Loss: 13354.5579, Val Loss: 3090986.0000\n",
            "Epoch [424/1000], Train Loss: 14164.5702, Val Loss: 3049038.8750\n",
            "Epoch [425/1000], Train Loss: 14711.4474, Val Loss: 3090855.8750\n",
            "Epoch [426/1000], Train Loss: 12860.2093, Val Loss: 3061964.8750\n",
            "Epoch [427/1000], Train Loss: 11373.7644, Val Loss: 3095889.7500\n",
            "Epoch [428/1000], Train Loss: 12548.7672, Val Loss: 3086142.5000\n",
            "Epoch [429/1000], Train Loss: 11424.6041, Val Loss: 3064837.0000\n",
            "Epoch [430/1000], Train Loss: 12097.7291, Val Loss: 3094386.1250\n",
            "Epoch [431/1000], Train Loss: 11766.9535, Val Loss: 3045127.1250\n",
            "Epoch [432/1000], Train Loss: 11665.3930, Val Loss: 3079235.8750\n",
            "Epoch [433/1000], Train Loss: 11552.8058, Val Loss: 3055475.6250\n",
            "Epoch [434/1000], Train Loss: 13145.3150, Val Loss: 3092479.2500\n",
            "Epoch [435/1000], Train Loss: 12666.5219, Val Loss: 3074650.6250\n",
            "Epoch [436/1000], Train Loss: 10384.8066, Val Loss: 3074424.6250\n",
            "Epoch [437/1000], Train Loss: 11059.5898, Val Loss: 3047575.0000\n",
            "Epoch [438/1000], Train Loss: 10378.9682, Val Loss: 3083113.0000\n",
            "Epoch [439/1000], Train Loss: 10502.5804, Val Loss: 3055064.6250\n",
            "Epoch [440/1000], Train Loss: 10152.5939, Val Loss: 3061115.5000\n",
            "Epoch [441/1000], Train Loss: 9656.0523, Val Loss: 3066506.0000\n",
            "Epoch [442/1000], Train Loss: 10976.9803, Val Loss: 3065846.3750\n",
            "Epoch [443/1000], Train Loss: 8475.4665, Val Loss: 3087020.8750\n",
            "Epoch [444/1000], Train Loss: 8472.8285, Val Loss: 3078588.3750\n",
            "Epoch [445/1000], Train Loss: 10005.1699, Val Loss: 3062571.7500\n",
            "Epoch [446/1000], Train Loss: 12172.1426, Val Loss: 3093017.1250\n",
            "Epoch [447/1000], Train Loss: 10937.4241, Val Loss: 3084107.1250\n",
            "Epoch [448/1000], Train Loss: 10520.1411, Val Loss: 3056307.8750\n",
            "Epoch [449/1000], Train Loss: 8514.0826, Val Loss: 3069432.8750\n",
            "Epoch [450/1000], Train Loss: 7410.2468, Val Loss: 3078135.7500\n",
            "Epoch [451/1000], Train Loss: 7793.4586, Val Loss: 3083677.6250\n",
            "Epoch [452/1000], Train Loss: 8299.4768, Val Loss: 3088677.7500\n",
            "Epoch [453/1000], Train Loss: 9125.8646, Val Loss: 3061089.0000\n",
            "Epoch [454/1000], Train Loss: 8498.7560, Val Loss: 3062606.3750\n",
            "Epoch [455/1000], Train Loss: 6481.4354, Val Loss: 3062400.3750\n",
            "Epoch [456/1000], Train Loss: 6464.6772, Val Loss: 3076099.2500\n",
            "Epoch [457/1000], Train Loss: 6478.0758, Val Loss: 3077300.0000\n",
            "Epoch [458/1000], Train Loss: 6451.5615, Val Loss: 3074228.6250\n",
            "Epoch [459/1000], Train Loss: 6378.3680, Val Loss: 3070274.0000\n",
            "Epoch [460/1000], Train Loss: 5978.5082, Val Loss: 3066890.6250\n",
            "Epoch [461/1000], Train Loss: 5980.9761, Val Loss: 3083256.5000\n",
            "Epoch [462/1000], Train Loss: 6747.5278, Val Loss: 3054710.7500\n",
            "Epoch [463/1000], Train Loss: 8605.5977, Val Loss: 3052347.3750\n",
            "Epoch [464/1000], Train Loss: 11435.9482, Val Loss: 3081002.2500\n",
            "Epoch [465/1000], Train Loss: 10991.1830, Val Loss: 3069714.3750\n",
            "Epoch [466/1000], Train Loss: 6132.0946, Val Loss: 3067917.3750\n",
            "Epoch [467/1000], Train Loss: 5886.1283, Val Loss: 3071098.0000\n",
            "Epoch [468/1000], Train Loss: 5668.1289, Val Loss: 3073495.2500\n",
            "Epoch [469/1000], Train Loss: 5266.5114, Val Loss: 3083205.2500\n",
            "Epoch [470/1000], Train Loss: 5055.9208, Val Loss: 3073385.7500\n",
            "Epoch [471/1000], Train Loss: 4641.0120, Val Loss: 3079284.5000\n",
            "Epoch [472/1000], Train Loss: 5002.0142, Val Loss: 3098426.3750\n",
            "Epoch [473/1000], Train Loss: 6914.6979, Val Loss: 3081220.7500\n",
            "Epoch [474/1000], Train Loss: 6830.7021, Val Loss: 3075820.3750\n",
            "Epoch [475/1000], Train Loss: 6146.8920, Val Loss: 3068928.1250\n",
            "Epoch [476/1000], Train Loss: 6747.7852, Val Loss: 3064964.5000\n",
            "Epoch [477/1000], Train Loss: 6345.4895, Val Loss: 3087495.0000\n",
            "Epoch [478/1000], Train Loss: 6706.3319, Val Loss: 3086587.2500\n",
            "Epoch [479/1000], Train Loss: 5170.8558, Val Loss: 3071501.0000\n",
            "Epoch [480/1000], Train Loss: 5477.7889, Val Loss: 3074528.7500\n",
            "Epoch [481/1000], Train Loss: 5074.3349, Val Loss: 3092433.3750\n",
            "Epoch [482/1000], Train Loss: 4513.1683, Val Loss: 3071988.3750\n",
            "Epoch [483/1000], Train Loss: 4282.6684, Val Loss: 3071962.7500\n",
            "Epoch [484/1000], Train Loss: 3866.3098, Val Loss: 3070011.3750\n",
            "Epoch [485/1000], Train Loss: 4022.9471, Val Loss: 3077241.8750\n",
            "Epoch [486/1000], Train Loss: 3842.4353, Val Loss: 3075412.6250\n",
            "Epoch [487/1000], Train Loss: 4644.3900, Val Loss: 3071915.2500\n",
            "Epoch [488/1000], Train Loss: 4661.9209, Val Loss: 3075166.3750\n",
            "Epoch [489/1000], Train Loss: 4307.0153, Val Loss: 3075341.8750\n",
            "Epoch [490/1000], Train Loss: 4765.9276, Val Loss: 3071700.8750\n",
            "Epoch [491/1000], Train Loss: 3636.0883, Val Loss: 3084395.3750\n",
            "Epoch [492/1000], Train Loss: 3457.8361, Val Loss: 3092123.6250\n",
            "Epoch [493/1000], Train Loss: 3535.5240, Val Loss: 3079503.6250\n",
            "Epoch [494/1000], Train Loss: 3151.8541, Val Loss: 3077630.7500\n",
            "Epoch [495/1000], Train Loss: 2827.3941, Val Loss: 3084679.5000\n",
            "Epoch [496/1000], Train Loss: 2727.0084, Val Loss: 3087346.7500\n",
            "Epoch [497/1000], Train Loss: 2893.1497, Val Loss: 3069356.0000\n",
            "Epoch [498/1000], Train Loss: 3862.9787, Val Loss: 3062117.3750\n",
            "Epoch [499/1000], Train Loss: 5419.0235, Val Loss: 3068981.1250\n",
            "Epoch [500/1000], Train Loss: 4423.8951, Val Loss: 3072632.8750\n",
            "Epoch [501/1000], Train Loss: 2809.3396, Val Loss: 3067913.1250\n",
            "Epoch [502/1000], Train Loss: 2920.8316, Val Loss: 3097959.5000\n",
            "Epoch [503/1000], Train Loss: 4754.7110, Val Loss: 3112841.5000\n",
            "Epoch [504/1000], Train Loss: 4978.2932, Val Loss: 3099439.2500\n",
            "Epoch [505/1000], Train Loss: 3030.0576, Val Loss: 3098402.2500\n",
            "Epoch [506/1000], Train Loss: 2802.5927, Val Loss: 3088903.7500\n",
            "Epoch [507/1000], Train Loss: 2958.1307, Val Loss: 3087325.6250\n",
            "Epoch [508/1000], Train Loss: 2564.6089, Val Loss: 3080167.6250\n",
            "Epoch [509/1000], Train Loss: 2397.3298, Val Loss: 3073784.2500\n",
            "Epoch [510/1000], Train Loss: 2403.8685, Val Loss: 3095118.7500\n",
            "Epoch [511/1000], Train Loss: 2363.5161, Val Loss: 3096007.2500\n",
            "Epoch [512/1000], Train Loss: 2766.3459, Val Loss: 3092360.8750\n",
            "Epoch [513/1000], Train Loss: 2226.6751, Val Loss: 3110313.0000\n",
            "Epoch [514/1000], Train Loss: 2952.4871, Val Loss: 3093654.5000\n",
            "Epoch [515/1000], Train Loss: 2818.8924, Val Loss: 3103213.2500\n",
            "Epoch [516/1000], Train Loss: 2964.5648, Val Loss: 3092181.7500\n",
            "Epoch [517/1000], Train Loss: 2410.2415, Val Loss: 3080421.8750\n",
            "Epoch [518/1000], Train Loss: 2127.8958, Val Loss: 3075141.5000\n",
            "Epoch [519/1000], Train Loss: 2864.0981, Val Loss: 3073425.0000\n",
            "Epoch [520/1000], Train Loss: 4440.3670, Val Loss: 3090397.7500\n",
            "Epoch [521/1000], Train Loss: 2653.3345, Val Loss: 3085444.2500\n",
            "Epoch [522/1000], Train Loss: 2680.7690, Val Loss: 3085046.6250\n",
            "Epoch [523/1000], Train Loss: 2885.0179, Val Loss: 3082719.2500\n",
            "Epoch [524/1000], Train Loss: 1956.8954, Val Loss: 3090428.2500\n",
            "Epoch [525/1000], Train Loss: 2735.8363, Val Loss: 3095653.7500\n",
            "Epoch [526/1000], Train Loss: 3250.4113, Val Loss: 3081828.2500\n",
            "Epoch [527/1000], Train Loss: 2046.8681, Val Loss: 3075496.6250\n",
            "Epoch [528/1000], Train Loss: 1850.1896, Val Loss: 3083274.1250\n",
            "Epoch [529/1000], Train Loss: 1505.2840, Val Loss: 3099252.0000\n",
            "Epoch [530/1000], Train Loss: 2282.6183, Val Loss: 3086421.8750\n",
            "Epoch [531/1000], Train Loss: 1472.9444, Val Loss: 3091736.3750\n",
            "Epoch [532/1000], Train Loss: 1486.4048, Val Loss: 3095932.8750\n",
            "Epoch [533/1000], Train Loss: 1856.7032, Val Loss: 3078703.7500\n",
            "Epoch [534/1000], Train Loss: 2459.4508, Val Loss: 3073066.6250\n",
            "Epoch [535/1000], Train Loss: 3215.4180, Val Loss: 3087119.6250\n",
            "Epoch [536/1000], Train Loss: 3424.2817, Val Loss: 3118721.0000\n",
            "Epoch [537/1000], Train Loss: 3701.5787, Val Loss: 3121731.7500\n",
            "Epoch [538/1000], Train Loss: 3372.3186, Val Loss: 3100825.2500\n",
            "Epoch [539/1000], Train Loss: 3250.6990, Val Loss: 3079059.0000\n",
            "Epoch [540/1000], Train Loss: 2729.3416, Val Loss: 3078648.6250\n",
            "Epoch [541/1000], Train Loss: 2260.7287, Val Loss: 3104382.0000\n",
            "Epoch [542/1000], Train Loss: 1711.2102, Val Loss: 3080017.6250\n",
            "Epoch [543/1000], Train Loss: 1305.3009, Val Loss: 3086914.1250\n",
            "Epoch [544/1000], Train Loss: 1328.4744, Val Loss: 3102301.7500\n",
            "Epoch [545/1000], Train Loss: 1217.4743, Val Loss: 3090535.5000\n",
            "Epoch [546/1000], Train Loss: 1900.9818, Val Loss: 3097679.7500\n",
            "Epoch [547/1000], Train Loss: 1736.9940, Val Loss: 3105103.0000\n",
            "Epoch [548/1000], Train Loss: 1637.1376, Val Loss: 3129872.8750\n",
            "Epoch [549/1000], Train Loss: 2952.8857, Val Loss: 3111406.3750\n",
            "Epoch [550/1000], Train Loss: 2905.8398, Val Loss: 3090060.3750\n",
            "Epoch [551/1000], Train Loss: 4360.6842, Val Loss: 3094019.3750\n",
            "Epoch [552/1000], Train Loss: 3400.8280, Val Loss: 3112767.3750\n",
            "Epoch [553/1000], Train Loss: 1948.8790, Val Loss: 3107381.6250\n",
            "Epoch [554/1000], Train Loss: 1202.9319, Val Loss: 3101124.0000\n",
            "Epoch [555/1000], Train Loss: 935.0503, Val Loss: 3091695.8750\n",
            "Epoch [556/1000], Train Loss: 1057.3785, Val Loss: 3100510.5000\n",
            "Epoch [557/1000], Train Loss: 845.0528, Val Loss: 3098031.6250\n",
            "Epoch [558/1000], Train Loss: 952.6192, Val Loss: 3089699.6250\n",
            "Epoch [559/1000], Train Loss: 1684.7626, Val Loss: 3117612.8750\n",
            "Epoch [560/1000], Train Loss: 2546.1608, Val Loss: 3113367.6250\n",
            "Epoch [561/1000], Train Loss: 2216.7419, Val Loss: 3092804.0000\n",
            "Epoch [562/1000], Train Loss: 1603.2818, Val Loss: 3098539.1250\n",
            "Epoch [563/1000], Train Loss: 1359.7480, Val Loss: 3113142.5000\n",
            "Epoch [564/1000], Train Loss: 1544.2588, Val Loss: 3088124.8750\n",
            "Epoch [565/1000], Train Loss: 1388.0760, Val Loss: 3088658.2500\n",
            "Epoch [566/1000], Train Loss: 1419.2296, Val Loss: 3100099.6250\n",
            "Epoch [567/1000], Train Loss: 1970.5111, Val Loss: 3091719.6250\n",
            "Epoch [568/1000], Train Loss: 1524.5123, Val Loss: 3090882.0000\n",
            "Epoch [569/1000], Train Loss: 1020.4158, Val Loss: 3099330.8750\n",
            "Epoch [570/1000], Train Loss: 804.8605, Val Loss: 3096830.2500\n",
            "Epoch [571/1000], Train Loss: 773.6338, Val Loss: 3091608.2500\n",
            "Epoch [572/1000], Train Loss: 776.7064, Val Loss: 3101251.8750\n",
            "Epoch [573/1000], Train Loss: 547.1279, Val Loss: 3098612.0000\n",
            "Epoch [574/1000], Train Loss: 469.9131, Val Loss: 3107626.2500\n",
            "Epoch [575/1000], Train Loss: 543.9715, Val Loss: 3097440.2500\n",
            "Epoch [576/1000], Train Loss: 708.2953, Val Loss: 3106730.8750\n",
            "Epoch [577/1000], Train Loss: 736.0220, Val Loss: 3098386.7500\n",
            "Epoch [578/1000], Train Loss: 718.9176, Val Loss: 3083748.2500\n",
            "Epoch [579/1000], Train Loss: 1674.9470, Val Loss: 3115800.8750\n",
            "Epoch [580/1000], Train Loss: 1402.0904, Val Loss: 3085489.0000\n",
            "Epoch [581/1000], Train Loss: 1639.6970, Val Loss: 3121336.8750\n",
            "Epoch [582/1000], Train Loss: 1844.0862, Val Loss: 3090336.6250\n",
            "Epoch [583/1000], Train Loss: 1440.4306, Val Loss: 3104673.7500\n",
            "Epoch [584/1000], Train Loss: 1171.8274, Val Loss: 3084918.7500\n",
            "Epoch [585/1000], Train Loss: 1706.4475, Val Loss: 3112056.1250\n",
            "Epoch [586/1000], Train Loss: 1083.1832, Val Loss: 3090222.1250\n",
            "Epoch [587/1000], Train Loss: 657.0346, Val Loss: 3098901.3750\n",
            "Epoch [588/1000], Train Loss: 495.7531, Val Loss: 3095303.2500\n",
            "Epoch [589/1000], Train Loss: 539.8484, Val Loss: 3105364.3750\n",
            "Epoch [590/1000], Train Loss: 377.6761, Val Loss: 3100955.2500\n",
            "Epoch [591/1000], Train Loss: 381.5125, Val Loss: 3103657.2500\n",
            "Epoch [592/1000], Train Loss: 407.7083, Val Loss: 3105874.8750\n",
            "Epoch [593/1000], Train Loss: 454.7032, Val Loss: 3098859.0000\n",
            "Epoch [594/1000], Train Loss: 377.1720, Val Loss: 3095139.2500\n",
            "Epoch [595/1000], Train Loss: 688.9604, Val Loss: 3105187.7500\n",
            "Epoch [596/1000], Train Loss: 337.3452, Val Loss: 3107443.8750\n",
            "Epoch [597/1000], Train Loss: 426.1261, Val Loss: 3113826.1250\n",
            "Epoch [598/1000], Train Loss: 458.8984, Val Loss: 3097532.6250\n",
            "Epoch [599/1000], Train Loss: 532.1662, Val Loss: 3119577.6250\n",
            "Epoch [600/1000], Train Loss: 956.6573, Val Loss: 3097309.5000\n",
            "Epoch [601/1000], Train Loss: 1614.0668, Val Loss: 3106027.3750\n",
            "Epoch [602/1000], Train Loss: 661.4846, Val Loss: 3096201.3750\n",
            "Epoch [603/1000], Train Loss: 653.1888, Val Loss: 3095656.6250\n",
            "Epoch [604/1000], Train Loss: 1342.4590, Val Loss: 3095955.2500\n",
            "Epoch [605/1000], Train Loss: 801.9281, Val Loss: 3111280.0000\n",
            "Epoch [606/1000], Train Loss: 704.4671, Val Loss: 3098776.0000\n",
            "Epoch [607/1000], Train Loss: 483.3563, Val Loss: 3106761.0000\n",
            "Epoch [608/1000], Train Loss: 423.8352, Val Loss: 3109844.1250\n",
            "Epoch [609/1000], Train Loss: 454.2932, Val Loss: 3105252.6250\n",
            "Epoch [610/1000], Train Loss: 352.6238, Val Loss: 3112957.8750\n",
            "Epoch [611/1000], Train Loss: 532.4476, Val Loss: 3097823.0000\n",
            "Epoch [612/1000], Train Loss: 548.6131, Val Loss: 3101999.7500\n",
            "Epoch [613/1000], Train Loss: 332.8078, Val Loss: 3102426.6250\n",
            "Epoch [614/1000], Train Loss: 464.8934, Val Loss: 3108960.8750\n",
            "Epoch [615/1000], Train Loss: 900.7509, Val Loss: 3104115.0000\n",
            "Epoch [616/1000], Train Loss: 671.1447, Val Loss: 3104231.2500\n",
            "Epoch [617/1000], Train Loss: 380.4356, Val Loss: 3097777.5000\n",
            "Epoch [618/1000], Train Loss: 797.1413, Val Loss: 3103611.0000\n",
            "Epoch [619/1000], Train Loss: 573.8241, Val Loss: 3113665.2500\n",
            "Epoch [620/1000], Train Loss: 874.1793, Val Loss: 3082732.1250\n",
            "Epoch [621/1000], Train Loss: 2279.0270, Val Loss: 3100762.2500\n",
            "Epoch [622/1000], Train Loss: 773.7623, Val Loss: 3129620.6250\n",
            "Epoch [623/1000], Train Loss: 1748.4845, Val Loss: 3109278.1250\n",
            "Epoch [624/1000], Train Loss: 1138.7936, Val Loss: 3092578.2500\n",
            "Epoch [625/1000], Train Loss: 3944.5338, Val Loss: 3087986.2500\n",
            "Epoch [626/1000], Train Loss: 5354.1774, Val Loss: 3103982.1250\n",
            "Epoch [627/1000], Train Loss: 4518.0451, Val Loss: 3191209.3750\n",
            "Epoch [628/1000], Train Loss: 20005.9417, Val Loss: 3127069.5000\n",
            "Epoch [629/1000], Train Loss: 12315.6511, Val Loss: 3103646.5000\n",
            "Epoch [630/1000], Train Loss: 18802.1797, Val Loss: 3106934.1250\n",
            "Epoch [631/1000], Train Loss: 24008.1504, Val Loss: 3217890.0000\n",
            "Epoch [632/1000], Train Loss: 24984.9099, Val Loss: 3141906.7500\n",
            "Epoch [633/1000], Train Loss: 18217.8057, Val Loss: 3096679.2500\n",
            "Epoch [634/1000], Train Loss: 16291.8976, Val Loss: 3120038.1250\n",
            "Epoch [635/1000], Train Loss: 11801.4102, Val Loss: 3045961.5000\n",
            "Epoch [636/1000], Train Loss: 15762.2381, Val Loss: 3156450.8750\n",
            "Epoch [637/1000], Train Loss: 16545.1781, Val Loss: 3067126.5000\n",
            "Epoch [638/1000], Train Loss: 21567.0127, Val Loss: 3091660.7500\n",
            "Epoch [639/1000], Train Loss: 15067.1735, Val Loss: 3070958.2500\n",
            "Epoch [640/1000], Train Loss: 15576.0439, Val Loss: 3173221.0000\n",
            "Epoch [641/1000], Train Loss: 11905.1007, Val Loss: 3100136.6250\n",
            "Epoch [642/1000], Train Loss: 6374.8894, Val Loss: 3090510.5000\n",
            "Epoch [643/1000], Train Loss: 6721.2228, Val Loss: 3091759.5000\n",
            "Epoch [644/1000], Train Loss: 4233.7461, Val Loss: 3115705.3750\n",
            "Epoch [645/1000], Train Loss: 2745.2502, Val Loss: 3111062.0000\n",
            "Epoch [646/1000], Train Loss: 1936.8892, Val Loss: 3106512.7500\n",
            "Epoch [647/1000], Train Loss: 1378.2508, Val Loss: 3099425.3750\n",
            "Epoch [648/1000], Train Loss: 1222.3043, Val Loss: 3090926.2500\n",
            "Epoch [649/1000], Train Loss: 4367.1774, Val Loss: 3108406.5000\n",
            "Epoch [650/1000], Train Loss: 8362.7175, Val Loss: 3123143.2500\n",
            "Epoch [651/1000], Train Loss: 7233.9452, Val Loss: 3093105.8750\n",
            "Epoch [652/1000], Train Loss: 6357.7252, Val Loss: 3100875.5000\n",
            "Epoch [653/1000], Train Loss: 3232.5086, Val Loss: 3092528.0000\n",
            "Epoch [654/1000], Train Loss: 2406.8811, Val Loss: 3097413.3750\n",
            "Epoch [655/1000], Train Loss: 2461.0788, Val Loss: 3094241.7500\n",
            "Epoch [656/1000], Train Loss: 1378.4921, Val Loss: 3108908.1250\n",
            "Epoch [657/1000], Train Loss: 1398.3490, Val Loss: 3099875.7500\n",
            "Epoch [658/1000], Train Loss: 1929.7835, Val Loss: 3120749.5000\n",
            "Epoch [659/1000], Train Loss: 1022.5701, Val Loss: 3097457.7500\n",
            "Epoch [660/1000], Train Loss: 1037.1523, Val Loss: 3113539.1250\n",
            "Epoch [661/1000], Train Loss: 1106.3874, Val Loss: 3103511.7500\n",
            "Epoch [662/1000], Train Loss: 814.4501, Val Loss: 3107428.6250\n",
            "Epoch [663/1000], Train Loss: 676.9976, Val Loss: 3106759.0000\n",
            "Epoch [664/1000], Train Loss: 464.6849, Val Loss: 3109752.5000\n",
            "Epoch [665/1000], Train Loss: 459.2203, Val Loss: 3103463.1250\n",
            "Epoch [666/1000], Train Loss: 454.6132, Val Loss: 3114480.1250\n",
            "Epoch [667/1000], Train Loss: 639.1748, Val Loss: 3112792.0000\n",
            "Epoch [668/1000], Train Loss: 478.6149, Val Loss: 3108452.1250\n",
            "Epoch [669/1000], Train Loss: 1022.4920, Val Loss: 3106725.0000\n",
            "Epoch [670/1000], Train Loss: 714.3290, Val Loss: 3110058.7500\n",
            "Epoch [671/1000], Train Loss: 481.6674, Val Loss: 3114998.6250\n",
            "Epoch [672/1000], Train Loss: 630.3775, Val Loss: 3120660.0000\n",
            "Epoch [673/1000], Train Loss: 677.6053, Val Loss: 3102777.1250\n",
            "Epoch [674/1000], Train Loss: 503.3143, Val Loss: 3115657.0000\n",
            "Epoch [675/1000], Train Loss: 276.6585, Val Loss: 3107625.3750\n",
            "Epoch [676/1000], Train Loss: 189.8496, Val Loss: 3106642.5000\n",
            "Epoch [677/1000], Train Loss: 211.4184, Val Loss: 3107814.7500\n",
            "Epoch [678/1000], Train Loss: 310.5666, Val Loss: 3122980.2500\n",
            "Epoch [679/1000], Train Loss: 1425.2274, Val Loss: 3129368.0000\n",
            "Epoch [680/1000], Train Loss: 2672.3393, Val Loss: 3094915.5000\n",
            "Epoch [681/1000], Train Loss: 4705.8119, Val Loss: 3096605.1250\n",
            "Epoch [682/1000], Train Loss: 2181.4837, Val Loss: 3151972.8750\n",
            "Epoch [683/1000], Train Loss: 4268.0011, Val Loss: 3116113.7500\n",
            "Epoch [684/1000], Train Loss: 3948.6821, Val Loss: 3112859.8750\n",
            "Epoch [685/1000], Train Loss: 2980.5935, Val Loss: 3106431.3750\n",
            "Epoch [686/1000], Train Loss: 6878.1645, Val Loss: 3140678.1250\n",
            "Epoch [687/1000], Train Loss: 16115.0171, Val Loss: 3117332.7500\n",
            "Epoch [688/1000], Train Loss: 14192.3339, Val Loss: 3070898.8750\n",
            "Epoch [689/1000], Train Loss: 32207.6600, Val Loss: 3149751.2500\n",
            "Epoch [690/1000], Train Loss: 21995.4245, Val Loss: 3205856.6250\n",
            "Epoch [691/1000], Train Loss: 14702.1763, Val Loss: 3148145.7500\n",
            "Epoch [692/1000], Train Loss: 23999.7694, Val Loss: 3095173.1250\n",
            "Epoch [693/1000], Train Loss: 89464.3814, Val Loss: 3273349.2500\n",
            "Epoch [694/1000], Train Loss: 103020.3376, Val Loss: 3171773.7500\n",
            "Epoch [695/1000], Train Loss: 85978.5541, Val Loss: 3158130.2500\n",
            "Epoch [696/1000], Train Loss: 23098.5500, Val Loss: 3109106.6250\n",
            "Epoch [697/1000], Train Loss: 12936.7871, Val Loss: 3066830.1250\n",
            "Epoch [698/1000], Train Loss: 7503.2145, Val Loss: 3100122.6250\n",
            "Epoch [699/1000], Train Loss: 5804.5396, Val Loss: 3121585.8750\n",
            "Epoch [700/1000], Train Loss: 3794.8844, Val Loss: 3145838.3750\n",
            "Epoch [701/1000], Train Loss: 3103.0772, Val Loss: 3139399.8750\n",
            "Epoch [702/1000], Train Loss: 3131.5042, Val Loss: 3105018.6250\n",
            "Epoch [703/1000], Train Loss: 5136.3614, Val Loss: 3104332.0000\n",
            "Epoch [704/1000], Train Loss: 3482.6153, Val Loss: 3120650.8750\n",
            "Epoch [705/1000], Train Loss: 2025.5451, Val Loss: 3108379.0000\n",
            "Epoch [706/1000], Train Loss: 1344.6356, Val Loss: 3096910.7500\n",
            "Epoch [707/1000], Train Loss: 2852.7490, Val Loss: 3134322.8750\n",
            "Epoch [708/1000], Train Loss: 3325.9894, Val Loss: 3118035.8750\n",
            "Epoch [709/1000], Train Loss: 2765.2701, Val Loss: 3108770.5000\n",
            "Epoch [710/1000], Train Loss: 4896.0403, Val Loss: 3152116.0000\n",
            "Epoch [711/1000], Train Loss: 3138.7038, Val Loss: 3151654.8750\n",
            "Epoch [712/1000], Train Loss: 8543.3476, Val Loss: 3083564.1250\n",
            "Epoch [713/1000], Train Loss: 17156.6031, Val Loss: 3112169.3750\n",
            "Epoch [714/1000], Train Loss: 5486.5596, Val Loss: 3168639.0000\n",
            "Epoch [715/1000], Train Loss: 4063.8566, Val Loss: 3097747.3750\n",
            "Epoch [716/1000], Train Loss: 2115.1400, Val Loss: 3105111.6250\n",
            "Epoch [717/1000], Train Loss: 2422.2977, Val Loss: 3108317.1250\n",
            "Epoch [718/1000], Train Loss: 2309.9994, Val Loss: 3119577.3750\n",
            "Epoch [719/1000], Train Loss: 1282.9616, Val Loss: 3119601.6250\n",
            "Epoch [720/1000], Train Loss: 831.4166, Val Loss: 3111541.2500\n",
            "Epoch [721/1000], Train Loss: 675.0501, Val Loss: 3109205.5000\n",
            "Epoch [722/1000], Train Loss: 473.5408, Val Loss: 3113237.0000\n",
            "Epoch [723/1000], Train Loss: 389.9030, Val Loss: 3102501.2500\n",
            "Epoch [724/1000], Train Loss: 1337.3869, Val Loss: 3126562.8750\n",
            "Epoch [725/1000], Train Loss: 819.6608, Val Loss: 3113418.5000\n",
            "Epoch [726/1000], Train Loss: 372.6769, Val Loss: 3106761.8750\n",
            "Epoch [727/1000], Train Loss: 647.3532, Val Loss: 3114775.3750\n",
            "Epoch [728/1000], Train Loss: 380.0928, Val Loss: 3105252.5000\n",
            "Epoch [729/1000], Train Loss: 228.4896, Val Loss: 3117583.0000\n",
            "Epoch [730/1000], Train Loss: 334.8255, Val Loss: 3105377.6250\n",
            "Epoch [731/1000], Train Loss: 157.7257, Val Loss: 3112341.1250\n",
            "Epoch [732/1000], Train Loss: 122.3507, Val Loss: 3109077.8750\n",
            "Epoch [733/1000], Train Loss: 87.3256, Val Loss: 3112041.2500\n",
            "Epoch [734/1000], Train Loss: 80.1875, Val Loss: 3110745.8750\n",
            "Epoch [735/1000], Train Loss: 164.4528, Val Loss: 3110098.1250\n",
            "Epoch [736/1000], Train Loss: 217.0105, Val Loss: 3105770.5000\n",
            "Epoch [737/1000], Train Loss: 155.5084, Val Loss: 3107862.0000\n",
            "Epoch [738/1000], Train Loss: 157.4376, Val Loss: 3107591.2500\n",
            "Epoch [739/1000], Train Loss: 137.1050, Val Loss: 3109782.5000\n",
            "Epoch [740/1000], Train Loss: 131.2736, Val Loss: 3106541.2500\n",
            "Epoch [741/1000], Train Loss: 201.6588, Val Loss: 3112188.3750\n",
            "Epoch [742/1000], Train Loss: 332.1144, Val Loss: 3106022.6250\n",
            "Epoch [743/1000], Train Loss: 235.9994, Val Loss: 3116453.2500\n",
            "Epoch [744/1000], Train Loss: 190.0897, Val Loss: 3105060.5000\n",
            "Epoch [745/1000], Train Loss: 363.8890, Val Loss: 3113672.3750\n",
            "Epoch [746/1000], Train Loss: 575.0315, Val Loss: 3113649.2500\n",
            "Epoch [747/1000], Train Loss: 251.2308, Val Loss: 3116505.5000\n",
            "Epoch [748/1000], Train Loss: 533.4367, Val Loss: 3110651.8750\n",
            "Epoch [749/1000], Train Loss: 259.1409, Val Loss: 3107388.5000\n",
            "Epoch [750/1000], Train Loss: 231.4901, Val Loss: 3106873.5000\n",
            "Epoch [751/1000], Train Loss: 375.7467, Val Loss: 3110917.5000\n",
            "Epoch [752/1000], Train Loss: 184.7212, Val Loss: 3117767.6250\n",
            "Epoch [753/1000], Train Loss: 149.9296, Val Loss: 3109673.7500\n",
            "Epoch [754/1000], Train Loss: 137.4534, Val Loss: 3112938.1250\n",
            "Epoch [755/1000], Train Loss: 231.4771, Val Loss: 3110451.2500\n",
            "Epoch [756/1000], Train Loss: 83.7805, Val Loss: 3113053.2500\n",
            "Epoch [757/1000], Train Loss: 85.4508, Val Loss: 3110703.3750\n",
            "Epoch [758/1000], Train Loss: 68.5672, Val Loss: 3110902.7500\n",
            "Epoch [759/1000], Train Loss: 81.2751, Val Loss: 3112659.5000\n",
            "Epoch [760/1000], Train Loss: 117.3093, Val Loss: 3112390.8750\n",
            "Epoch [761/1000], Train Loss: 59.4828, Val Loss: 3107294.0000\n",
            "Epoch [762/1000], Train Loss: 62.2821, Val Loss: 3112700.5000\n",
            "Epoch [763/1000], Train Loss: 87.5424, Val Loss: 3110258.1250\n",
            "Epoch [764/1000], Train Loss: 66.6769, Val Loss: 3108605.3750\n",
            "Epoch [765/1000], Train Loss: 62.0148, Val Loss: 3111174.0000\n",
            "Epoch [766/1000], Train Loss: 33.6726, Val Loss: 3113808.2500\n",
            "Epoch [767/1000], Train Loss: 27.1225, Val Loss: 3113365.1250\n",
            "Epoch [768/1000], Train Loss: 29.9967, Val Loss: 3112799.2500\n",
            "Epoch [769/1000], Train Loss: 38.2608, Val Loss: 3112071.3750\n",
            "Epoch [770/1000], Train Loss: 43.1112, Val Loss: 3110291.3750\n",
            "Epoch [771/1000], Train Loss: 48.7773, Val Loss: 3107827.6250\n",
            "Epoch [772/1000], Train Loss: 134.4932, Val Loss: 3111450.5000\n",
            "Epoch [773/1000], Train Loss: 215.7495, Val Loss: 3120547.3750\n",
            "Epoch [774/1000], Train Loss: 350.5931, Val Loss: 3112513.7500\n",
            "Epoch [775/1000], Train Loss: 559.3323, Val Loss: 3129363.3750\n",
            "Epoch [776/1000], Train Loss: 3073.3954, Val Loss: 3148209.0000\n",
            "Epoch [777/1000], Train Loss: 8228.0670, Val Loss: 3116799.3750\n",
            "Epoch [778/1000], Train Loss: 14578.4110, Val Loss: 3122948.2500\n",
            "Epoch [779/1000], Train Loss: 6442.5857, Val Loss: 3137533.0000\n",
            "Epoch [780/1000], Train Loss: 20507.8734, Val Loss: 3157665.1250\n",
            "Epoch [781/1000], Train Loss: 16679.1143, Val Loss: 3149910.5000\n",
            "Epoch [782/1000], Train Loss: 8427.0951, Val Loss: 3096614.1250\n",
            "Epoch [783/1000], Train Loss: 12280.4067, Val Loss: 3107348.1250\n",
            "Epoch [784/1000], Train Loss: 17626.6763, Val Loss: 3123341.3750\n",
            "Epoch [785/1000], Train Loss: 11433.9186, Val Loss: 3110623.0000\n",
            "Epoch [786/1000], Train Loss: 12611.3260, Val Loss: 3173771.8750\n",
            "Epoch [787/1000], Train Loss: 7471.7944, Val Loss: 3106143.0000\n",
            "Epoch [788/1000], Train Loss: 6950.7309, Val Loss: 3094935.6250\n",
            "Epoch [789/1000], Train Loss: 8435.9993, Val Loss: 3162488.5000\n",
            "Epoch [790/1000], Train Loss: 15142.8140, Val Loss: 3190441.7500\n",
            "Epoch [791/1000], Train Loss: 16222.4290, Val Loss: 3070122.7500\n",
            "Epoch [792/1000], Train Loss: 27858.1535, Val Loss: 3202934.1250\n",
            "Epoch [793/1000], Train Loss: 42452.3814, Val Loss: 3026832.8750\n",
            "Epoch [794/1000], Train Loss: 34322.0233, Val Loss: 3107368.0000\n",
            "Epoch [795/1000], Train Loss: 14332.1076, Val Loss: 3121626.3750\n",
            "Epoch [796/1000], Train Loss: 7772.9572, Val Loss: 3103320.7500\n",
            "Epoch [797/1000], Train Loss: 6332.6307, Val Loss: 3157710.1250\n",
            "Epoch [798/1000], Train Loss: 10682.3924, Val Loss: 3081192.6250\n",
            "Epoch [799/1000], Train Loss: 9760.9634, Val Loss: 3167186.1250\n",
            "Epoch [800/1000], Train Loss: 9915.6036, Val Loss: 3091787.7500\n",
            "Epoch [801/1000], Train Loss: 4920.4482, Val Loss: 3128041.0000\n",
            "Epoch [802/1000], Train Loss: 7647.7992, Val Loss: 3074888.0000\n",
            "Epoch [803/1000], Train Loss: 11966.8180, Val Loss: 3158009.1250\n",
            "Epoch [804/1000], Train Loss: 16020.2224, Val Loss: 3062786.0000\n",
            "Epoch [805/1000], Train Loss: 16279.1606, Val Loss: 3150535.5000\n",
            "Epoch [806/1000], Train Loss: 4902.1062, Val Loss: 3139289.2500\n",
            "Epoch [807/1000], Train Loss: 4883.1872, Val Loss: 3134413.5000\n",
            "Epoch [808/1000], Train Loss: 3842.0386, Val Loss: 3097523.6250\n",
            "Epoch [809/1000], Train Loss: 2937.8576, Val Loss: 3130959.3750\n",
            "Epoch [810/1000], Train Loss: 2260.9789, Val Loss: 3121529.1250\n",
            "Epoch [811/1000], Train Loss: 2617.9276, Val Loss: 3095007.7500\n",
            "Epoch [812/1000], Train Loss: 3731.6796, Val Loss: 3135109.1250\n",
            "Epoch [813/1000], Train Loss: 3671.1495, Val Loss: 3100618.8750\n",
            "Epoch [814/1000], Train Loss: 1929.7531, Val Loss: 3120008.5000\n",
            "Epoch [815/1000], Train Loss: 1656.3317, Val Loss: 3095936.1250\n",
            "Epoch [816/1000], Train Loss: 1238.9664, Val Loss: 3097426.2500\n",
            "Epoch [817/1000], Train Loss: 1974.6999, Val Loss: 3123366.5000\n",
            "Epoch [818/1000], Train Loss: 3287.9452, Val Loss: 3110637.5000\n",
            "Epoch [819/1000], Train Loss: 3137.8234, Val Loss: 3095202.6250\n",
            "Epoch [820/1000], Train Loss: 2161.8341, Val Loss: 3092361.8750\n",
            "Epoch [821/1000], Train Loss: 1003.6371, Val Loss: 3096074.8750\n",
            "Epoch [822/1000], Train Loss: 710.1359, Val Loss: 3101953.5000\n",
            "Epoch [823/1000], Train Loss: 484.4748, Val Loss: 3095732.2500\n",
            "Epoch [824/1000], Train Loss: 677.7451, Val Loss: 3108824.1250\n",
            "Epoch [825/1000], Train Loss: 598.2168, Val Loss: 3102175.1250\n",
            "Epoch [826/1000], Train Loss: 2398.2516, Val Loss: 3102624.1250\n",
            "Epoch [827/1000], Train Loss: 3801.2874, Val Loss: 3129320.5000\n",
            "Epoch [828/1000], Train Loss: 4015.0852, Val Loss: 3110174.1250\n",
            "Epoch [829/1000], Train Loss: 4726.4254, Val Loss: 3086075.2500\n",
            "Epoch [830/1000], Train Loss: 2295.5549, Val Loss: 3094126.0000\n",
            "Epoch [831/1000], Train Loss: 1241.5563, Val Loss: 3110665.0000\n",
            "Epoch [832/1000], Train Loss: 1861.9371, Val Loss: 3121820.7500\n",
            "Epoch [833/1000], Train Loss: 3390.5710, Val Loss: 3090021.5000\n",
            "Epoch [834/1000], Train Loss: 3074.0563, Val Loss: 3125228.2500\n",
            "Epoch [835/1000], Train Loss: 4364.8032, Val Loss: 3094707.5000\n",
            "Epoch [836/1000], Train Loss: 2215.9348, Val Loss: 3096881.0000\n",
            "Epoch [837/1000], Train Loss: 1831.9214, Val Loss: 3117466.2500\n",
            "Epoch [838/1000], Train Loss: 3287.2864, Val Loss: 3102497.0000\n",
            "Epoch [839/1000], Train Loss: 2115.1685, Val Loss: 3095447.7500\n",
            "Epoch [840/1000], Train Loss: 4826.7214, Val Loss: 3111990.3750\n",
            "Epoch [841/1000], Train Loss: 2200.2328, Val Loss: 3092394.7500\n",
            "Epoch [842/1000], Train Loss: 1379.8851, Val Loss: 3098332.6250\n",
            "Epoch [843/1000], Train Loss: 1324.2966, Val Loss: 3127965.1250\n",
            "Epoch [844/1000], Train Loss: 1727.7550, Val Loss: 3093262.1250\n",
            "Epoch [845/1000], Train Loss: 3269.5411, Val Loss: 3113919.2500\n",
            "Epoch [846/1000], Train Loss: 13043.1444, Val Loss: 3142145.7500\n",
            "Epoch [847/1000], Train Loss: 8420.4328, Val Loss: 3107923.3750\n",
            "Epoch [848/1000], Train Loss: 4991.5330, Val Loss: 3119105.3750\n",
            "Epoch [849/1000], Train Loss: 3314.0213, Val Loss: 3084119.3750\n",
            "Epoch [850/1000], Train Loss: 2328.7256, Val Loss: 3101764.1250\n",
            "Epoch [851/1000], Train Loss: 1788.8052, Val Loss: 3089836.7500\n",
            "Epoch [852/1000], Train Loss: 4163.5200, Val Loss: 3088743.1250\n",
            "Epoch [853/1000], Train Loss: 6669.6568, Val Loss: 3175533.8750\n",
            "Epoch [854/1000], Train Loss: 16010.6790, Val Loss: 3120495.1250\n",
            "Epoch [855/1000], Train Loss: 7969.8639, Val Loss: 3110436.0000\n",
            "Epoch [856/1000], Train Loss: 3737.0645, Val Loss: 3136991.3750\n",
            "Epoch [857/1000], Train Loss: 3936.0334, Val Loss: 3084797.2500\n",
            "Epoch [858/1000], Train Loss: 2687.5086, Val Loss: 3097940.5000\n",
            "Epoch [859/1000], Train Loss: 6782.2709, Val Loss: 3223404.5000\n",
            "Epoch [860/1000], Train Loss: 25118.8053, Val Loss: 3128150.8750\n",
            "Epoch [861/1000], Train Loss: 35963.5893, Val Loss: 3086999.8750\n",
            "Epoch [862/1000], Train Loss: 54169.0268, Val Loss: 3204043.2500\n",
            "Epoch [863/1000], Train Loss: 48926.5818, Val Loss: 3203098.1250\n",
            "Epoch [864/1000], Train Loss: 32997.8641, Val Loss: 3113688.2500\n",
            "Epoch [865/1000], Train Loss: 21550.4735, Val Loss: 3123402.5000\n",
            "Epoch [866/1000], Train Loss: 24001.8546, Val Loss: 3100148.0000\n",
            "Epoch [867/1000], Train Loss: 36525.9840, Val Loss: 3247661.2500\n",
            "Epoch [868/1000], Train Loss: 25762.4255, Val Loss: 3094017.3750\n",
            "Epoch [869/1000], Train Loss: 11636.5502, Val Loss: 3070426.5000\n",
            "Epoch [870/1000], Train Loss: 18846.9450, Val Loss: 3139178.0000\n",
            "Epoch [871/1000], Train Loss: 6576.4067, Val Loss: 3139663.3750\n",
            "Epoch [872/1000], Train Loss: 13105.5149, Val Loss: 3089951.1250\n",
            "Epoch [873/1000], Train Loss: 17005.9325, Val Loss: 3096969.5000\n",
            "Epoch [874/1000], Train Loss: 11745.6524, Val Loss: 3130419.7500\n",
            "Epoch [875/1000], Train Loss: 5587.4962, Val Loss: 3087057.0000\n",
            "Epoch [876/1000], Train Loss: 3419.4188, Val Loss: 3069114.2500\n",
            "Epoch [877/1000], Train Loss: 4795.0243, Val Loss: 3105706.6250\n",
            "Epoch [878/1000], Train Loss: 3247.2354, Val Loss: 3094373.3750\n",
            "Epoch [879/1000], Train Loss: 2454.3976, Val Loss: 3095143.3750\n",
            "Epoch [880/1000], Train Loss: 981.4081, Val Loss: 3108127.8750\n",
            "Epoch [881/1000], Train Loss: 1132.5337, Val Loss: 3085769.8750\n",
            "Epoch [882/1000], Train Loss: 784.6263, Val Loss: 3095488.3750\n",
            "Epoch [883/1000], Train Loss: 512.5023, Val Loss: 3092725.2500\n",
            "Epoch [884/1000], Train Loss: 374.6913, Val Loss: 3094323.2500\n",
            "Epoch [885/1000], Train Loss: 510.0019, Val Loss: 3084996.5000\n",
            "Epoch [886/1000], Train Loss: 448.6336, Val Loss: 3100522.1250\n",
            "Epoch [887/1000], Train Loss: 367.1198, Val Loss: 3093716.7500\n",
            "Epoch [888/1000], Train Loss: 249.7088, Val Loss: 3087693.2500\n",
            "Epoch [889/1000], Train Loss: 325.3869, Val Loss: 3086735.6250\n",
            "Epoch [890/1000], Train Loss: 574.7707, Val Loss: 3096941.7500\n",
            "Epoch [891/1000], Train Loss: 762.5307, Val Loss: 3082061.2500\n",
            "Epoch [892/1000], Train Loss: 582.3490, Val Loss: 3102915.6250\n",
            "Epoch [893/1000], Train Loss: 1090.1832, Val Loss: 3091539.1250\n",
            "Epoch [894/1000], Train Loss: 1059.6971, Val Loss: 3094138.3750\n",
            "Epoch [895/1000], Train Loss: 913.4458, Val Loss: 3109645.8750\n",
            "Epoch [896/1000], Train Loss: 2064.9376, Val Loss: 3085812.0000\n",
            "Epoch [897/1000], Train Loss: 2059.9224, Val Loss: 3117202.7500\n",
            "Epoch [898/1000], Train Loss: 1000.1208, Val Loss: 3099662.5000\n",
            "Epoch [899/1000], Train Loss: 1001.3724, Val Loss: 3107773.5000\n",
            "Epoch [900/1000], Train Loss: 797.7273, Val Loss: 3077297.8750\n",
            "Epoch [901/1000], Train Loss: 897.9652, Val Loss: 3107521.2500\n",
            "Epoch [902/1000], Train Loss: 2194.5957, Val Loss: 3079877.3750\n",
            "Epoch [903/1000], Train Loss: 1524.1953, Val Loss: 3112525.2500\n",
            "Epoch [904/1000], Train Loss: 781.5369, Val Loss: 3096208.5000\n",
            "Epoch [905/1000], Train Loss: 329.8325, Val Loss: 3094272.6250\n",
            "Epoch [906/1000], Train Loss: 386.8880, Val Loss: 3099165.0000\n",
            "Epoch [907/1000], Train Loss: 473.4540, Val Loss: 3091939.6250\n",
            "Epoch [908/1000], Train Loss: 280.9439, Val Loss: 3098358.5000\n",
            "Epoch [909/1000], Train Loss: 278.4296, Val Loss: 3088984.5000\n",
            "Epoch [910/1000], Train Loss: 386.0431, Val Loss: 3104981.2500\n",
            "Epoch [911/1000], Train Loss: 590.7089, Val Loss: 3084926.5000\n",
            "Epoch [912/1000], Train Loss: 245.2348, Val Loss: 3086887.0000\n",
            "Epoch [913/1000], Train Loss: 322.4453, Val Loss: 3084680.6250\n",
            "Epoch [914/1000], Train Loss: 452.5873, Val Loss: 3098426.7500\n",
            "Epoch [915/1000], Train Loss: 654.5578, Val Loss: 3091145.7500\n",
            "Epoch [916/1000], Train Loss: 264.7161, Val Loss: 3097413.1250\n",
            "Epoch [917/1000], Train Loss: 455.2873, Val Loss: 3088309.6250\n",
            "Epoch [918/1000], Train Loss: 246.1405, Val Loss: 3096621.5000\n",
            "Epoch [919/1000], Train Loss: 561.3691, Val Loss: 3093441.7500\n",
            "Epoch [920/1000], Train Loss: 489.9892, Val Loss: 3091590.6250\n",
            "Epoch [921/1000], Train Loss: 742.1976, Val Loss: 3123474.0000\n",
            "Epoch [922/1000], Train Loss: 3832.0913, Val Loss: 3096731.0000\n",
            "Epoch [923/1000], Train Loss: 1489.2303, Val Loss: 3082916.1250\n",
            "Epoch [924/1000], Train Loss: 1848.3702, Val Loss: 3095248.1250\n",
            "Epoch [925/1000], Train Loss: 869.1728, Val Loss: 3101000.7500\n",
            "Epoch [926/1000], Train Loss: 1065.6881, Val Loss: 3102502.8750\n",
            "Epoch [927/1000], Train Loss: 1342.0189, Val Loss: 3085876.6250\n",
            "Epoch [928/1000], Train Loss: 494.7012, Val Loss: 3094818.3750\n",
            "Epoch [929/1000], Train Loss: 566.9325, Val Loss: 3089078.8750\n",
            "Epoch [930/1000], Train Loss: 966.9612, Val Loss: 3093795.1250\n",
            "Epoch [931/1000], Train Loss: 664.6793, Val Loss: 3087229.3750\n",
            "Epoch [932/1000], Train Loss: 551.0045, Val Loss: 3087676.7500\n",
            "Epoch [933/1000], Train Loss: 355.2992, Val Loss: 3087086.5000\n",
            "Epoch [934/1000], Train Loss: 493.2138, Val Loss: 3103544.2500\n",
            "Epoch [935/1000], Train Loss: 581.0511, Val Loss: 3101182.8750\n",
            "Epoch [936/1000], Train Loss: 618.2020, Val Loss: 3077746.8750\n",
            "Epoch [937/1000], Train Loss: 1551.6626, Val Loss: 3087527.8750\n",
            "Epoch [938/1000], Train Loss: 1036.6790, Val Loss: 3106424.1250\n",
            "Epoch [939/1000], Train Loss: 964.9751, Val Loss: 3098584.0000\n",
            "Epoch [940/1000], Train Loss: 728.6666, Val Loss: 3092614.8750\n",
            "Epoch [941/1000], Train Loss: 1208.6034, Val Loss: 3087299.1250\n",
            "Epoch [942/1000], Train Loss: 2793.4910, Val Loss: 3119302.7500\n",
            "Epoch [943/1000], Train Loss: 8009.8488, Val Loss: 3092519.2500\n",
            "Epoch [944/1000], Train Loss: 5795.3705, Val Loss: 3133531.1250\n",
            "Epoch [945/1000], Train Loss: 4598.7090, Val Loss: 3099407.2500\n",
            "Epoch [946/1000], Train Loss: 2682.0423, Val Loss: 3087434.8750\n",
            "Epoch [947/1000], Train Loss: 2490.0188, Val Loss: 3106834.6250\n",
            "Epoch [948/1000], Train Loss: 4901.6172, Val Loss: 3049138.3750\n",
            "Epoch [949/1000], Train Loss: 6879.6596, Val Loss: 3049518.0000\n",
            "Epoch [950/1000], Train Loss: 5996.1310, Val Loss: 3103397.8750\n",
            "Epoch [951/1000], Train Loss: 11039.2276, Val Loss: 3092955.2500\n",
            "Epoch [952/1000], Train Loss: 4812.1382, Val Loss: 3053547.8750\n",
            "Epoch [953/1000], Train Loss: 17185.7955, Val Loss: 3084484.0000\n",
            "Epoch [954/1000], Train Loss: 7877.6228, Val Loss: 3136314.8750\n",
            "Epoch [955/1000], Train Loss: 22967.2743, Val Loss: 3262583.0000\n",
            "Epoch [956/1000], Train Loss: 34179.4614, Val Loss: 3096380.0000\n",
            "Epoch [957/1000], Train Loss: 24218.0190, Val Loss: 3104832.5000\n",
            "Epoch [958/1000], Train Loss: 11665.5590, Val Loss: 3110977.2500\n",
            "Epoch [959/1000], Train Loss: 10075.3794, Val Loss: 3058386.1250\n",
            "Epoch [960/1000], Train Loss: 24131.7427, Val Loss: 3043803.6250\n",
            "Epoch [961/1000], Train Loss: 25053.5149, Val Loss: 3157205.8750\n",
            "Epoch [962/1000], Train Loss: 17654.1413, Val Loss: 3050002.0000\n",
            "Epoch [963/1000], Train Loss: 21678.3343, Val Loss: 3318268.3750\n",
            "Epoch [964/1000], Train Loss: 67373.7860, Val Loss: 3122844.6250\n",
            "Epoch [965/1000], Train Loss: 68750.5597, Val Loss: 3447587.0000\n",
            "Epoch [966/1000], Train Loss: 93673.6281, Val Loss: 3069000.3750\n",
            "Epoch [967/1000], Train Loss: 57570.7882, Val Loss: 3058697.7500\n",
            "Epoch [968/1000], Train Loss: 49207.1437, Val Loss: 3147091.2500\n",
            "Epoch [969/1000], Train Loss: 30719.1063, Val Loss: 3145390.0000\n",
            "Epoch [970/1000], Train Loss: 17222.8428, Val Loss: 3088010.1250\n",
            "Epoch [971/1000], Train Loss: 16585.0254, Val Loss: 3092220.2500\n",
            "Epoch [972/1000], Train Loss: 16436.2663, Val Loss: 3074335.2500\n",
            "Epoch [973/1000], Train Loss: 24388.5558, Val Loss: 3089005.2500\n",
            "Epoch [974/1000], Train Loss: 33740.4890, Val Loss: 3077625.5000\n",
            "Epoch [975/1000], Train Loss: 18308.5587, Val Loss: 3140456.2500\n",
            "Epoch [976/1000], Train Loss: 13854.8717, Val Loss: 3134070.0000\n",
            "Epoch [977/1000], Train Loss: 9707.8528, Val Loss: 3165496.6250\n",
            "Epoch [978/1000], Train Loss: 23289.2839, Val Loss: 3086996.6250\n",
            "Epoch [979/1000], Train Loss: 13174.7393, Val Loss: 3058438.0000\n",
            "Epoch [980/1000], Train Loss: 7217.0852, Val Loss: 3116360.5000\n",
            "Epoch [981/1000], Train Loss: 6199.7564, Val Loss: 3059541.7500\n",
            "Epoch [982/1000], Train Loss: 4057.4774, Val Loss: 3063080.8750\n",
            "Epoch [983/1000], Train Loss: 2552.9851, Val Loss: 3082995.5000\n",
            "Epoch [984/1000], Train Loss: 1475.0696, Val Loss: 3089467.0000\n",
            "Epoch [985/1000], Train Loss: 1913.4985, Val Loss: 3071179.2500\n",
            "Epoch [986/1000], Train Loss: 1333.1251, Val Loss: 3080211.1250\n",
            "Epoch [987/1000], Train Loss: 802.9024, Val Loss: 3083726.0000\n",
            "Epoch [988/1000], Train Loss: 1155.3436, Val Loss: 3081744.0000\n",
            "Epoch [989/1000], Train Loss: 1066.5615, Val Loss: 3071883.8750\n",
            "Epoch [990/1000], Train Loss: 886.2365, Val Loss: 3075806.1250\n",
            "Epoch [991/1000], Train Loss: 582.2816, Val Loss: 3072350.6250\n",
            "Epoch [992/1000], Train Loss: 480.1013, Val Loss: 3083650.2500\n",
            "Epoch [993/1000], Train Loss: 534.2300, Val Loss: 3078519.0000\n",
            "Epoch [994/1000], Train Loss: 525.5630, Val Loss: 3073108.3750\n",
            "Epoch [995/1000], Train Loss: 399.2380, Val Loss: 3068508.7500\n",
            "Epoch [996/1000], Train Loss: 562.8703, Val Loss: 3070041.0000\n",
            "Epoch [997/1000], Train Loss: 673.6788, Val Loss: 3082905.3750\n",
            "Epoch [998/1000], Train Loss: 401.2007, Val Loss: 3079706.5000\n",
            "Epoch [999/1000], Train Loss: 377.7400, Val Loss: 3064145.0000\n",
            "Epoch [1000/1000], Train Loss: 727.5045, Val Loss: 3080589.8750\n",
            "--------------------------------------------------------\n",
            "model saved.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 數據轉換函數\n",
        "def convert_base_into_list(number, base, width=10):\n",
        "    result = []\n",
        "    for i in range(width):\n",
        "        result.append(number % base)\n",
        "        number = number // base\n",
        "    return result[::-1]  # 返回扁平的列表\n",
        "\n",
        "print(\"---loading data---\")\n",
        "# 加載數據\n",
        "trainData = pd.read_csv('/content/Drive/MyDrive/實驗/goldbach/csv/traindata_min_size_1000.csv')\n",
        "trainDataNumbers = trainData['Number'].values\n",
        "trainDataLabel = trainData['Factors'].values\n",
        "print(\"---loading data done.---\")\n",
        "\n",
        "# 打亂訓練數據\n",
        "shuffleIndices = np.random.permutation(len(trainDataNumbers))\n",
        "shuffledNumbers = trainDataNumbers[shuffleIndices]\n",
        "shuffledLabel = trainDataLabel[shuffleIndices]\n",
        "\n",
        "# 準備特徵\n",
        "mergedTrainNumber = []\n",
        "for num in shuffledNumbers:\n",
        "    feature = convert_base_into_list(num, 2) + convert_base_into_list(num, 3) + \\\n",
        "              convert_base_into_list(num, 5) + convert_base_into_list(num, 7)\n",
        "    mergedTrainNumber.append(feature)\n",
        "\n",
        "mergedArray = np.array(mergedTrainNumber)\n",
        "mergedLabel = np.array(shuffledLabel)\n",
        "\n",
        "# 分割數據\n",
        "X_train, X_val, y_train, y_val = train_test_split(mergedArray, mergedLabel, test_size=0.2, random_state=42)\n",
        "\n",
        "# 創建 PyTorch 數據集和數據加載器\n",
        "class GoldbachDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.FloatTensor(labels).view(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = GoldbachDataset(X_train, y_train)\n",
        "val_dataset = GoldbachDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "# 定義模型\n",
        "class GoldbachModel(nn.Module):\n",
        "    def __init__(self, input_size=40):\n",
        "        super(GoldbachModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# 檢查是否有可用的GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"--- Using device: {device} ---\")\n",
        "\n",
        "# 創建模型實例\n",
        "input_size = X_train.shape[1]\n",
        "print(f\"Input size: {input_size}\")\n",
        "model = GoldbachModel(input_size).to(device)\n",
        "\n",
        "# 定義損失函數和優化器\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 訓練模型\n",
        "print(\"---starting training---\")\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 驗證\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in val_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            val_loss += criterion(outputs, batch_labels).item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), 'goldbach_model_0725.pth')\n",
        "print(\"model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training"
      ],
      "metadata": {
        "id": "ylbXzmBJmQRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI_u090kmqfv",
        "outputId": "e5b20cfb-7551-46cb-e0bb-9a56d8888c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (800, 40, 1)\n",
            "Shape of y_train: (800,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練模型\n",
        "print(\"---starting training---\")\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs.squeeze(), batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 驗證\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in val_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            val_loss += criterion(outputs.squeeze(), batch_labels).item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), 'goldbach_model_1020_400w.pth')\n",
        "print(\"model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "LkXkTZimmQAG",
        "outputId": "6f1321f1-bfac-475a-98f5-e638fb397a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---starting training---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([800])) that is different to the input size (torch.Size([800, 40])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (40) must match the size of tensor b (800) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e6157d0a8802>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3363\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (40) must match the size of tensor b (800) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict"
      ],
      "metadata": {
        "id": "Kf9iJZEn-cZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nbn4CLgI-I-",
        "outputId": "5f31c5c5-1a4f-4280-ae22-0f4cfb086078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loadding prime table\n",
        "primeTable = pd.read_csv(\"/content/Drive/MyDrive/實驗/goldbach/csv/prime_table_5000000.csv\")\n",
        "primeTableNumber = primeTable[\"primes\"].values\n",
        "print (\"---loading primes table---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c8mDF4dJ9YC",
        "outputId": "69fc7de8-66b7-4c38-8872-6af1a798efd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---loading primes table---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "#predict\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# load torch model\n",
        "model = GoldbachModel(input_size)\n",
        "model.load_state_dict(torch.load('goldbach_model_0725.pth'))\n",
        "\n",
        "# load test_set and prime data\n",
        "testDataPath = '/content/Drive/MyDrive/實驗/goldbach/csv/test_set.csv'\n",
        "testData = pd.read_csv(testDataPath)\n",
        "testDataNumbers = testData['Number'].values\n",
        "testDataLabel = testData['Factors'].values\n",
        "\n",
        "print(\"---loading test data done.---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHHJ3UKUI4Kd",
        "outputId": "c386b71d-f5b2-4898-9031-770e752b9007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---loading test data done.---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle training data\n",
        "shuffleIndices = np.random.permutation(len(testDataNumbers))\n",
        "shuffledNumbers = testDataNumbers[shuffleIndices]\n",
        "shuffledLabel = testDataLabel[shuffleIndices]"
      ],
      "metadata": {
        "id": "DdLMugz5KeQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate 25 prime table\n",
        "primeTable25 = primeTableNumber[:25]\n",
        "print (primeTable25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P016SagF3Ley",
        "outputId": "eb7b01af-f054-4221-bec7-5069c0dc9958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2  3  5  7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89\n",
            " 97]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## one number\n",
        "torch version"
      ],
      "metadata": {
        "id": "lyKZ7bnBrHuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 評估模型\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in val_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # # 假設是二分類問題，計算準確度\n",
        "        # predicted = (outputs > 0.5).float()\n",
        "        # total += batch_labels.size(0)\n",
        "        # correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "average_loss = total_loss / len(val_loader)\n",
        "\n",
        "print(f\"Loss: {average_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRjgKnpLrXAk",
        "outputId": "148db4f7-83c2-47f1-e7ab-b1e6e004b2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3080589.8750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 加載模型\n",
        "model = GoldbachModel(input_size)\n",
        "model.load_state_dict(torch.load('goldbach_model_0725.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 準備數據\n",
        "test_data = []  # 這裡放你想要預測的數字\n",
        "new_features = []\n",
        "for num in shuffledNumbers:\n",
        "    feature = convert_base_into_list(num, 2) + convert_base_into_list(num, 3) + \\\n",
        "              convert_base_into_list(num, 5) + convert_base_into_list(num, 7)\n",
        "    new_features.append(feature)\n",
        "\n",
        "\n",
        "\n",
        "new_features = torch.FloatTensor(new_features).to(device)\n",
        "\n",
        "# 進行預測\n",
        "with torch.no_grad():\n",
        "    predictions = model(new_features)\n",
        "\n",
        "# # 輸出結果\n",
        "print(predictions.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "oqEKGcPy6MWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dd6c32-c9ee-4daf-c0d7-2000db2f3494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15031.179]\n",
            " [36580.44 ]\n",
            " [30671.1  ]\n",
            " [23571.498]\n",
            " [36222.133]\n",
            " [14329.631]\n",
            " [35477.844]\n",
            " [19946.96 ]\n",
            " [17878.895]\n",
            " [13885.85 ]\n",
            " [17843.88 ]\n",
            " [33272.08 ]\n",
            " [35271.83 ]\n",
            " [40637.79 ]\n",
            " [16778.498]\n",
            " [34703.715]\n",
            " [19543.363]\n",
            " [35963.47 ]\n",
            " [15709.218]\n",
            " [21333.703]\n",
            " [25594.297]\n",
            " [16892.29 ]\n",
            " [22211.81 ]\n",
            " [17365.27 ]\n",
            " [30677.592]\n",
            " [19537.941]\n",
            " [17475.613]\n",
            " [19651.922]\n",
            " [14856.12 ]\n",
            " [43659.945]\n",
            " [16815.371]\n",
            " [20018.656]\n",
            " [20027.459]\n",
            " [18301.785]\n",
            " [14482.29 ]\n",
            " [18105.598]\n",
            " [17937.033]\n",
            " [21270.635]\n",
            " [18454.39 ]\n",
            " [35671.73 ]\n",
            " [17678.61 ]\n",
            " [18232.43 ]\n",
            " [36004.31 ]\n",
            " [20611.49 ]\n",
            " [21280.547]\n",
            " [34899.39 ]\n",
            " [16225.054]\n",
            " [18809.85 ]\n",
            " [22050.83 ]\n",
            " [31308.867]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roundPredictions = [int(i) for i in predictions]"
      ],
      "metadata": {
        "id": "03RniE6zvfHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (len(testDataLabel))\n",
        "print (len(roundPredictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbuGvXIwwQNq",
        "outputId": "91aa5457-4cf1-45f1-a3fb-9d969cde00aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EL1SkGuAwQGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mse(predictions, targets):\n",
        "    \"\"\"\n",
        "    計算均方誤差 (MSE)\n",
        "\n",
        "    參數:\n",
        "    predictions: 預測值的列表或陣列\n",
        "    targets: 實際值的列表或陣列，與預測值對應\n",
        "\n",
        "    返回值:\n",
        "    mse: 均方誤差\n",
        "    \"\"\"\n",
        "    # 確保預測值和實際值的長度相等\n",
        "    if len(predictions) != len(targets):\n",
        "        raise ValueError(\"預測值和實際值的長度不一致\")\n",
        "\n",
        "    # 計算平方誤差\n",
        "    squared_errors = [(p - t) ** 2 for p, t in zip(predictions, targets)]\n",
        "\n",
        "    # 計算均方誤差\n",
        "    mse = sum(squared_errors) / len(predictions)\n",
        "\n",
        "    return mse\n",
        "\n",
        "# 使用 LN\n",
        "mse_result = calculate_mse(roundPredictions, testDataLabel)\n",
        "\n",
        "# 使用 MLP(old)\n",
        "#mse_result = calculate_mse(predictions, testDataPartition)\n",
        "\n",
        "\n",
        "import math\n",
        "mean = testData['Factors'].mean()\n",
        "\n",
        "#+N MSE\n",
        "MSE = int(calculate_mse(roundPredictions, testDataLabel))\n",
        "\n",
        "#MLP MSE\n",
        "# MSE = int(calculate_mse(predictions, testDataPartition))\n",
        "\n",
        "RMSE= math.sqrt(MSE)\n",
        "errorRate = RMSE/mean\n",
        "\n",
        "print (f\"MSE : {MSE}\\nRMSE : {RMSE}\\nError Rate : {errorRate}  ->  {round(errorRate,7)*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Lbw7MWrLxS",
        "outputId": "7982670c-8f15-4eac-be2d-8324cdddf77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE : 139189400\n",
            "RMSE : 11797.85573737872\n",
            "Error Rate : 0.5608006902609586  ->  56.080070000000006%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AJQGxFVwH5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}